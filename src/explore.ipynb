{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c311b1",
   "metadata": {},
   "source": [
    "# EXPLORED EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# IMPORTS\n",
    "# -------------------------------\n",
    "\n",
    "# Main libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Checking correlation between attributes\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Optimize models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Plot models\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Save models\n",
    "from pickle import dump\n",
    "\n",
    "# CLASSIFICATION MODELS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# REGRESSION MODELS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402175fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 0) LOAD RAW DATAFRAME\n",
    "# -------------------------------\n",
    "print(\"STEP 0) LOAD RAW DATAFRAME\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "data_separator = \",\"\n",
    "input_path = \"../data/raw/internal-link.csv\"\n",
    "\n",
    "# Read DataFrame\n",
    "df_raw=pd.read_csv(input_path, sep = data_separator)\n",
    "\n",
    "print(\"- ‚úÖ DataFrame loaded sucessfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e7ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 1) EXPLORE DATAFRAME\n",
    "# -------------------------------\n",
    "print(\"STEP 1) EXPLORE DATAFRAME\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S1 = df_raw.copy()\n",
    "\n",
    "# Print shape\n",
    "print(f\"- ‚ÑπÔ∏è Shape of the original DataFrame: {df_S1.shape}\")\n",
    "\n",
    "# Show first rows\n",
    "print(\"- ‚ÑπÔ∏è Content of the original DataFrame:\")\n",
    "display(df_S1.head(5))\n",
    "\n",
    "# Show dataframe info\n",
    "print(\"- ‚ÑπÔ∏è Info of the original DataFrame (dataType and non-null values):\")\n",
    "df_S1.info(verbose=True, show_counts=True)\n",
    "\n",
    "# Ordered info (fewest non-null first)\n",
    "print(\"\\n- ‚ÑπÔ∏è Ordered info by number of non-null values:\")\n",
    "ordered_info = pd.DataFrame({\n",
    "    \"Column\": df_S1.columns,\n",
    "    \"Non-Null Count\": df_S1.notnull().sum(),\n",
    "    \"Null Count\": df_S1.isnull().sum(),\n",
    "    \"Dtype\": df_S1.dtypes.astype(str)\n",
    "}).sort_values(by=\"Non-Null Count\", ascending=True)\n",
    "\n",
    "display(ordered_info)\n",
    "\n",
    "# Count unique attributes (unsorted)\n",
    "df_S1_summary = pd.DataFrame({\n",
    "    \"Column\": df_S1.columns,\n",
    "    \"Unique_Count\": df_S1.nunique().values\n",
    "})\n",
    "print(\"\\n - ‚ÑπÔ∏è Final DataFrame unique attributes (unsorted):\")\n",
    "display(df_S1_summary)\n",
    "\n",
    "# Ordered summary (fewest unique values first)\n",
    "print(\"\\n - ‚ÑπÔ∏è Ordered unique attributes (fewest unique first):\")\n",
    "df_S1_summary_ordered = df_S1_summary.sort_values(by=\"Unique_Count\", ascending=True)\n",
    "display(df_S1_summary_ordered)\n",
    "\n",
    "# Automatic Warning for high-uniqueness columns\n",
    "unique_counts = df_S1.nunique()\n",
    "high_unique_cols = unique_counts[unique_counts == len(df_S1)].index.tolist()\n",
    "if len(high_unique_cols) > 0:\n",
    "    print(\"\\n - ‚ö†Ô∏è Consider dropping the following columns for having UNIQUE values for EVERY row:\")\n",
    "    for col in high_unique_cols:\n",
    "        print(f\"   - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d1c501",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- all my attributes are considered relevant\n",
    "- there are not non-null values in the data -> nice\n",
    "- Outcome is going to be the target variable of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8cc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 2) SELECT RELEVANT ATTRIBUTES\n",
    "# -------------------------------\n",
    "print(\"STEP 2) SELECT RELEVANT ATTRIBUTES\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S2 = df_S1.copy()\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "df_S2=df_S2.drop(labels=[], axis =1) # Drop non-relevant attributes\n",
    "\n",
    "# Print results\n",
    "print(\"- ‚úÖ Non-Relevant attributes have been dropped.\")\n",
    "print(f\" - ‚ÑπÔ∏è Previous df's columns: {len(df_S1.columns)}\")\n",
    "print(f\" - ‚ÑπÔ∏è Current df's columns: {len(df_S2.columns)}\")\n",
    "print(f\" - ‚ÑπÔ∏è Final DataFrame shape: {df_S2.shape}\")\n",
    "display(df_S2.head())\n",
    "\n",
    "# Count attributes\n",
    "df_S2_summary = pd.DataFrame({\n",
    "    \"Column\": df_S2.columns,\n",
    "    \"Unique_Count\": df_S2.nunique().values\n",
    "})\n",
    "print(\" - ‚ÑπÔ∏è Final DataFrame unique attributes:\")\n",
    "display(df_S2_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c38a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 3) REMOVE DUPLICATES\n",
    "# -------------------------------\n",
    "print(\"STEP 3) REMOVE DUPLICATES\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S3 = df_S2.copy()\n",
    "\n",
    "num_duplicates=df_S3.duplicated().sum()\n",
    "if num_duplicates == 0:\n",
    "    df_S3=df_S3\n",
    "    print(\"- ‚úÖ Previous DataFrame does not contain duplicates:\")\n",
    "    print(\" - ‚ÑπÔ∏è Previous df's shape: \",df_S2.shape)\n",
    "    print(\" - ‚ÑπÔ∏è Current df's  shape: \",df_S3.shape)\n",
    "    print(\" - ‚ÑπÔ∏è These are the dropped duplicates:\")\n",
    "else:\n",
    "    df_S3_duplicates=df_S3[df_S3.duplicated()] #Works as bool mask\n",
    "    df_S3=df_S3.drop_duplicates()\n",
    "    print(\"- ‚ö†Ô∏è Previous DataFrame contained \" + str(num_duplicates) + \" duplicates that have been dropped:\")\n",
    "    print(\" - ‚ÑπÔ∏è Previous df's shape: \",df_S2.shape)\n",
    "    print(\" - ‚ÑπÔ∏è Current df's  shape: \",df_S3.shape)\n",
    "    print(\" - ‚ÑπÔ∏è These are the dropped duplicates:\")\n",
    "    display(df_S3_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 4) CLASSIFY ATTRIBUTES AND TARGET VARIABLE\n",
    "# -------------------------------\n",
    "print(\"STEP 4) CLASSIFY ATTRIBUTES AND TARGET VARIABLE\")\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S4 = df_S3.copy()\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "var_type_proposal_threshold = 0.25  # [%] Under this percentage of unique values, the attribute is proposed as CATEGORIC\n",
    "float_discrete_threshold = min(30, round(0.02 * len(df_S4))) # Dynamic threshold for FLOAT to be considered DISCRETE\n",
    "\n",
    "# List of columns\n",
    "columns = df_S4.columns.tolist()\n",
    "\n",
    "# Iterate through columns\n",
    "category_var_auto = []\n",
    "numeric_var_auto = []\n",
    "for col in df_S4.columns:\n",
    "    col_data = df_S4[col].dropna()\n",
    "    total_rows = len(df_S4)\n",
    "\n",
    "    # Skip empty columns\n",
    "    if total_rows == 0:\n",
    "        continue\n",
    "\n",
    "    unique_count = col_data.nunique()\n",
    "    unique_ratio = unique_count / total_rows * 100\n",
    "    col_dtype = str(df_S4[col].dtype)\n",
    "\n",
    "    # Case 1: text-based columns\n",
    "    if col_dtype in [\"object\", \"category\"]:\n",
    "        category_var_auto.append(col)\n",
    "        continue\n",
    "\n",
    "    # Case 2: integer columns\n",
    "    if col_dtype.startswith(\"int\"):\n",
    "        if unique_ratio <= var_type_proposal_threshold:\n",
    "            category_var_auto.append(col)\n",
    "        else:\n",
    "            numeric_var_auto.append(col)\n",
    "        continue\n",
    "\n",
    "    # Case 3: float columns\n",
    "    if col_dtype.startswith(\"float\"):\n",
    "        if unique_ratio <= var_type_proposal_threshold:\n",
    "            category_var_auto.append(col)\n",
    "        else:\n",
    "            numeric_var_auto.append(col)\n",
    "        continue\n",
    "\n",
    "# Print proposed Data Types\n",
    "print(\"- ‚ÑπÔ∏è Proposed CATEGORY Attributes: \" + str(category_var_auto))\n",
    "print(\"- ‚ÑπÔ∏è Proposed NUMERIC Attributes: \" + str(numeric_var_auto))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6685b1b",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- Proposal of CATEGORY vs NUMERIC values matches perfectly the types given by the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7306f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "y_var = \"Outcome\" # Confirm target variable\n",
    "if_target_is_binary_treat_as_categoric = True # Confirm treatment for target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25211d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm categories and target variable\n",
    "category_att = []\n",
    "numeric_att = []\n",
    "for att in category_var_auto:\n",
    "    if att != y_var:\n",
    "        category_att.append(att)\n",
    "for att in numeric_var_auto:\n",
    "    if att != y_var:\n",
    "        numeric_att.append(att)\n",
    "\n",
    "# Checking CATEGORY attributes\n",
    "binary_att = []\n",
    "multiclass_att = []\n",
    "constant_att = []\n",
    "for att in category_att:\n",
    "    att_unique_values = df_S4[att].nunique()\n",
    "\n",
    "    if att_unique_values == 2:\n",
    "        binary_att.append(att)\n",
    "    elif att_unique_values > 2:\n",
    "        multiclass_att.append(att)\n",
    "    else:\n",
    "        constant_att.append(att)\n",
    "\n",
    "# Checking NUMERIC attributes\n",
    "discrete_att = []\n",
    "continuos_att = []\n",
    "for att in numeric_att:\n",
    "    att_dtype = df_S4[att].dtype.kind\n",
    "    unique_count = df_S4[att].nunique()\n",
    "\n",
    "    if att_dtype in ['i', 'u']:\n",
    "        discrete_att.append(att)\n",
    "    elif att_dtype == 'f' and unique_count < float_discrete_threshold:\n",
    "        discrete_att.append(att)\n",
    "    else:\n",
    "        continuos_att.append(att)\n",
    "\n",
    "# Checking TARGET variable\n",
    "y_unique_values = df_S4[y_var].nunique()\n",
    "y_dtype = df_S4[y_var].dtype.kind\n",
    "\n",
    "if y_var in category_var_auto:\n",
    "    if y_unique_values == 2:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"BINARY\"\n",
    "    elif y_unique_values > 2:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"MULTICLASS\"\n",
    "    else:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"CONSTANT\"\n",
    "else:\n",
    "    if y_unique_values == 2 and if_target_is_binary_treat_as_categoric:\n",
    "        y_var_type = \"CATEGORIC\"\n",
    "        y_var_subtype = \"BINARY\"\n",
    "    elif y_dtype in ['i', 'u']:\n",
    "        y_var_type = \"NUMERIC\"\n",
    "        y_var_subtype = \"DISCRETE\"\n",
    "    elif y_dtype == 'f' and y_unique_values < float_discrete_threshold:\n",
    "        y_var_type = \"NUMERIC\"\n",
    "        y_var_subtype = \"DISCRETE\"\n",
    "    else:\n",
    "        y_var_type = \"NUMERIC\"\n",
    "        y_var_subtype = \"CONTINUOUS\"\n",
    "\n",
    "# Print results\n",
    "print(\"- ‚ÑπÔ∏è Confirmed CATEGORY Attributes:\")\n",
    "print(\"   ‚Ü≥ BINARY: \" + str(binary_att))\n",
    "print(\"   ‚Ü≥ MULTICLASS: \" + str(multiclass_att))\n",
    "print(\"   ‚Ü≥ CONSTANT: \" + str(constant_att))\n",
    "print(\"- ‚ÑπÔ∏è Confirmed NUMERIC Attributes: \" + str(numeric_att))\n",
    "print(\"   ‚Ü≥ DISCRETE: \" + str(discrete_att))\n",
    "print(\"   ‚Ü≥ CONTINUOUS: \" + str(continuos_att))\n",
    "print(\"- ‚ÑπÔ∏è Confirmed TARGET Variable: \" + y_var + \" -> \" + y_var_type + \" and \" + y_var_subtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "make_plots_UNIVARIANT = True                # Draw plots?\n",
    "make_plots_MULTIVARIANT_TARGET = True       # Draw plots?\n",
    "make_plots_MULTIVARIANT_ATTRIBUTES = True   # Draw plots?\n",
    "figHeight_unit = 8                          # Unitary figure height\n",
    "figWidth_unit = 12                          # Unitary figure width\n",
    "num_cols = 2                                # Number of columns per plot\n",
    "my_palette = \"pastel\"                       # Palette\n",
    "my_font_size = 15                           # Font size\n",
    "font_size_titles = 25                       # Font size for titles\n",
    "num_values_to_plot = 40                     # Max number of different values to plot (for CATEGORY_var)\n",
    "num_bins = 100                              # Num of bins (for NUMERIC_var plots)\n",
    "category_combi_att = \"\"                     # Combination attribute for multivariant analysis (must be a CATEGORIC attribute)\n",
    "y_var_highlighting_color = \"green\"          # Color to highlight target variable\n",
    "\n",
    " # Validation\n",
    "if not category_att:\n",
    "    print(\"- ‚ÑπÔ∏è There are no CATEGORIC attributes in the DataFrame\")\n",
    "elif category_combi_att in category_att:\n",
    "    print(\"- ‚úÖ Sucessfull verification: combination attribute \" +  category_combi_att + \" is CATEGORIC\")\n",
    "elif category_combi_att in numeric_att:\n",
    "    raise ValueError(\"‚ùå Combination attribute \" +  category_combi_att + \" for multivariant analysis must be a CATEGORY attribute!\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Combination attribute \" +  category_combi_att + \" does not exist in the DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7250bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 5 - UNIVARIABLE ANALYSIS\n",
    "# -------------------------------\n",
    "print(\"STEP 5 - UNIVARIABLE ANALYSIS\")\n",
    "\n",
    "if not make_plots_UNIVARIANT:\n",
    "    print(\"‚ö†Ô∏è UNIVARIABLE ANALYSIS is not printed, set make_plots_UNIVARIANT = True\")\n",
    "else:\n",
    "\n",
    "    # Copy of previous DataFrame\n",
    "    df_S5 = df_S4.copy()\n",
    "\n",
    "    # Target highlighting styles\n",
    "    target_box_style = dict(facecolor='none', edgecolor=y_var_highlighting_color, linewidth=5)\n",
    "    target_title_style = dict(fontsize= font_size_titles, color=y_var_highlighting_color, fontweight='bold')\n",
    "\n",
    "    # CATEGORY VARIABLES (including target if categorical)\n",
    "    print(\"üè∑Ô∏è CATEGORY VARIABLES\")\n",
    "\n",
    "    if not category_att and y_var_type == \"NUMERIC\":\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because there are not CATEGORIC variables in the DataFrame\")\n",
    "    else:    \n",
    "        var_to_plot = category_att.copy()\n",
    "        if y_var_type == \"CATEGORIC\" and y_var not in var_to_plot:\n",
    "            var_to_plot.insert(0, y_var)\n",
    "\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=num_rows,\n",
    "            ncols=num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows)\n",
    "        )\n",
    "\n",
    "        axes = axes.flatten()\n",
    "        idx = 0\n",
    "\n",
    "        for col in var_to_plot:\n",
    "            unique_count = df_S5[col].nunique()\n",
    "\n",
    "            if unique_count > num_values_to_plot:\n",
    "                order = df_S5[col].value_counts().head(num_values_to_plot).index\n",
    "            else:\n",
    "                order = df_S5[col].value_counts().index\n",
    "\n",
    "            sns.countplot(\n",
    "                ax=axes[idx],\n",
    "                data=df_S5,\n",
    "                x=col,\n",
    "                hue=col,\n",
    "                palette=my_palette,\n",
    "                order=order,\n",
    "                legend=False\n",
    "            )\n",
    "            axes[idx].tick_params(axis='x', rotation=90, labelsize=my_font_size)\n",
    "            axes[idx].set_xlabel(\"\")\n",
    "\n",
    "            # Highlight target\n",
    "            if col == y_var:\n",
    "                axes[idx].set_title(col, **target_title_style)\n",
    "                axes[idx].add_patch(\n",
    "                    plt.Rectangle((0, 0), 1, 1, transform=axes[idx].transAxes, **target_box_style)\n",
    "                )\n",
    "            else:\n",
    "                axes[idx].set_title(col, fontdict = {\"fontsize\": font_size_titles})\n",
    "\n",
    "            # Add truncated info\n",
    "            if unique_count > num_values_to_plot:\n",
    "                msg = f\"There are {unique_count} values,\\nbut only {num_values_to_plot} have been plotted\"\n",
    "                axes[idx].text(\n",
    "                    0.5, 0.9, msg,\n",
    "                    transform=axes[idx].transAxes,\n",
    "                    fontsize=my_font_size,\n",
    "                    color=\"red\",\n",
    "                    ha=\"center\", va=\"top\",\n",
    "                    bbox=dict(facecolor=\"grey\", alpha=0.25, edgecolor=\"red\")\n",
    "                )\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "        # Hide unused axes\n",
    "        for j in range(idx, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # NUMERIC VARIABLES (including target if numeric)\n",
    "    print(\"üî¢ NUMERIC VARIABLES\")\n",
    "\n",
    "    if not numeric_att and y_var_type == \"CATEGORIC\":\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because there are not NUMERIC variables in the DataFrame\")\n",
    "    else: \n",
    "        var_to_plot = numeric_att.copy()\n",
    "        if y_var_type == \"NUMERIC\" and y_var not in var_to_plot:\n",
    "            var_to_plot.insert(0, y_var)\n",
    "\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols)\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=num_rows * 2,\n",
    "            ncols=num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'height_ratios': [4, 0.5] * num_rows}\n",
    "        )\n",
    "\n",
    "        var_idx = 0\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "\n",
    "                if var_idx >= len(var_to_plot):\n",
    "                    axes[row * 2, col].set_visible(False)\n",
    "                    axes[row * 2 + 1, col].set_visible(False)\n",
    "                    continue\n",
    "\n",
    "                colname = var_to_plot[var_idx]\n",
    "\n",
    "                # Histogram\n",
    "                sns.histplot(\n",
    "                    ax=axes[row * 2, col],\n",
    "                    data=df_S5,\n",
    "                    x=colname,\n",
    "                    bins=num_bins\n",
    "                )\n",
    "                axes[row * 2, col].set_xlabel(\"\")\n",
    "\n",
    "                # Boxplot\n",
    "                sns.boxplot(\n",
    "                    ax=axes[row * 2 + 1, col],\n",
    "                    data=df_S5,\n",
    "                    x=colname\n",
    "                )\n",
    "                axes[row * 2 + 1, col].set_xlabel(\"\")\n",
    "\n",
    "                # Highlight target\n",
    "                if colname == y_var:\n",
    "                    axes[row * 2, col].set_title(colname, **target_title_style)\n",
    "                    axes[row * 2 + 1, col].set_title(colname, **target_title_style)\n",
    "\n",
    "                    axes[row * 2, col].add_patch(\n",
    "                        plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2, col].transAxes, **target_box_style)\n",
    "                    )\n",
    "                    axes[row * 2 + 1, col].add_patch(\n",
    "                        plt.Rectangle((0, 0), 1, 1, transform=axes[row * 2 + 1, col].transAxes, **target_box_style)\n",
    "                    )\n",
    "                else:\n",
    "                    axes[row * 2, col].set_title(colname, fontdict = {\"fontsize\": font_size_titles})\n",
    "\n",
    "                var_idx += 1\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCLUSIONS:\n",
    "- there are around double negative than positive in diabetes\n",
    "- Glucose, BloodPressure, SkinThickness, Insulin, BMI present many zero values, which are impossible, so they will be taken as missing values\n",
    "- most of the data present outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094425ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 6 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET\n",
    "# -------------------------------\n",
    "print(\"STEP 6 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET\")\n",
    "\n",
    "if not make_plots_MULTIVARIANT_TARGET:\n",
    "    print(\"‚ö†Ô∏è MULTIVARIANT ANALYSIS - ATTRIBUTES VS TARGET is not printed, set make_plots_MULTIVARIANT_TARGET = True\")\n",
    "else:\n",
    "    # Copy  of previous DataFrame\n",
    "    df_S6 = df_S4.copy()\n",
    "    print(\"\\n üî¢ NUMERIC Attributes VS üè∑Ô∏è CATEGORY Target\")\n",
    "\n",
    "    if not numeric_att:\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because there are not NUMERIC attributes in the DataFrame\")\n",
    "    elif y_var_type == \"NUMERIC\":\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because the target variable is NUMERIC\")\n",
    "    else:\n",
    "        # Set plotting variables\n",
    "        var_to_plot=numeric_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots with custom height ratios\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows,\n",
    "            ncols = num_cols * 2,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'width_ratios': [3, 1] * num_cols})\n",
    "\n",
    "        # Loop through variables\n",
    "        var_idx = 0\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                if var_idx >= len(var_to_plot):\n",
    "                    # Hide unused subplots\n",
    "                    axes[row, col * 2].set_visible(False)\n",
    "                    axes[row , col* 2 + 1].set_visible(False)\n",
    "                    continue\n",
    "                \n",
    "                sns.stripplot(\n",
    "                    ax = axes[row, col * 2],\n",
    "                    data = df_S6,\n",
    "                    x = y_var,\n",
    "                    y = var_to_plot[var_idx],\n",
    "                    hue = y_var,\n",
    "                    alpha = 0.3,\n",
    "                    legend = False)\n",
    "                axes[row, col * 2].set_ylabel(var_to_plot[var_idx],fontdict = {\"fontsize\": my_font_size})\n",
    "                axes[row, col * 2].grid(True)\n",
    "\n",
    "                sns.boxplot(\n",
    "                    ax = axes[row, col * 2 + 1],\n",
    "                    data = df_S6,\n",
    "                    x = y_var,\n",
    "                    y = var_to_plot[var_idx],\n",
    "                    hue = y_var,\n",
    "                    palette = my_palette,\n",
    "                    legend = False)\n",
    "                axes[row, col * 2 + 1].set_ylabel(\"\")\n",
    "                axes[row, col * 2 + 1].grid(True)\n",
    "                axes[row, col * 2 + 1].set_yticklabels([])\n",
    "                \n",
    "                var_idx += 1\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"\\n üî¢ NUMERIC Attributes VS üî¢ NUMERIC Target\")\n",
    "\n",
    "    if not numeric_att:\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because there are not NUMERIC attributes in the DataFrame\")\n",
    "    elif y_var_type == \"CATEGORIC\":\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because the target variable is CATEGORIC\")\n",
    "    else:\n",
    "        # Set plotting variables\n",
    "        var_to_plot=numeric_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots with custom height ratios\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows * 2,\n",
    "            ncols = num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'height_ratios': [4, 1] * num_rows})\n",
    "\n",
    "        # Loop through variables\n",
    "        var_idx = 0\n",
    "        for row in range(num_rows):\n",
    "            for col in range(num_cols):\n",
    "                if var_idx >= len(var_to_plot):\n",
    "                    # Hide unused subplots\n",
    "                    axes[row * 2, col].set_visible(False)\n",
    "                    axes[row * 2 + 1, col].set_visible(False)\n",
    "                    continue\n",
    "\n",
    "                # Regplot (top)\n",
    "                sns.regplot(\n",
    "                    ax = axes[row * 2, col],\n",
    "                    data = df_S6,\n",
    "                    x = var_to_plot[var_idx],\n",
    "                    y = y_var,\n",
    "                    scatter_kws = {'s': my_font_size, 'alpha': 0.6},\n",
    "                    line_kws = {'color': 'red'})\n",
    "\n",
    "                # Boxplot (bottom)\n",
    "                sns.heatmap(\n",
    "                    ax = axes[row * 2 + 1, col],\n",
    "                    data = df_S6[[var_to_plot[var_idx], y_var]].corr(),\n",
    "                    annot = True,\n",
    "                    fmt = \".2f\",\n",
    "                    cbar = False)\n",
    "\n",
    "                var_idx += 1\n",
    "\n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n üè∑Ô∏è CATEGORY Attributes VS üî¢ NUMERIC Target\")\n",
    "\n",
    "    if not category_att:\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because there are not CATEGORIC attributes in the DataFrame\")\n",
    "    elif y_var_type == \"CATEGORIC\":\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because the target variable is CATEGORIC\")\n",
    "    else:        \n",
    "        # Set plotting variables\n",
    "        var_to_plot=category_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows,\n",
    "            ncols = num_cols,\n",
    "            figsize = (figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
    "\n",
    "        # flatten the axes array for easier access\n",
    "        axes = axes.flatten()  \n",
    "\n",
    "        # Loop through variables\n",
    "        idx = 0\n",
    "        for col in var_to_plot:\n",
    "\n",
    "            # Count unique values\n",
    "            unique_count = df_S6[col].nunique()\n",
    "            \n",
    "            # Limit the number of plotted categories if there are more than num_values_to_plot\n",
    "            if unique_count > num_values_to_plot:\n",
    "                order = df_S6[col].value_counts().head(num_values_to_plot).index\n",
    "            else:\n",
    "                order = df_S6[col].value_counts().index\n",
    "            \n",
    "            # Create barplot\n",
    "            sns.barplot(\n",
    "                ax=axes[idx],\n",
    "                data = df_S6,\n",
    "                x = col,\n",
    "                y = y_var,\n",
    "                hue = category_combi_att,\n",
    "                order = order)\n",
    "            axes[idx].tick_params(axis='x', rotation=90, labelsize=10)\n",
    "            \n",
    "            # Add text box if truncated\n",
    "            if unique_count > num_values_to_plot:\n",
    "                msg = \"There are \" + str(unique_count) + \" different values,\\nbut only \" + str(num_values_to_plot) + \" have been plotted\"\n",
    "                axes[idx].text(\n",
    "                    x = 0.5,\n",
    "                    y = 0.9,\n",
    "                    s = msg,\n",
    "                    transform=axes[idx].transAxes,\n",
    "                    fontsize = my_font_size,\n",
    "                    color = 'red',\n",
    "                    ha = 'center',\n",
    "                    va = 'top',\n",
    "                    bbox = dict(facecolor='grey', alpha=0.5, edgecolor='red'))\n",
    "            \n",
    "            idx += 1\n",
    "\n",
    "        # Turn off unused axes if there are any\n",
    "        for j in range(idx, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\n üè∑Ô∏è CATEGORY Attributes with üè∑Ô∏è Combined CATEGORY Target\")\n",
    "\n",
    "    if not category_att:\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because there are not CATEGORIC attributes in the DataFrame\")\n",
    "    elif y_var_type == \"NUMERIC\":\n",
    "        print(\"  ‚ö†Ô∏è This type of plot is non applicable for this case, because the target variable is NUMERIC\")\n",
    "    else:\n",
    "        # Set plotting variables\n",
    "        hue_order = sorted(df_S6[y_var].dropna().unique().tolist()) # Determine hue order dynamically\n",
    "        var_to_plot=category_att # Variable type for this plot\n",
    "        num_rows = math.ceil(len(var_to_plot) / num_cols) # Number of rows for the figure\n",
    "\n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows,\n",
    "            ncols = num_cols,\n",
    "            figsize=(figWidth_unit * num_cols, figHeight_unit * num_rows))\n",
    "            \n",
    "        # Flatten the axes array for easier access\n",
    "        axes = axes.flatten()  \n",
    "\n",
    "        # Loop through variables\n",
    "        idx = 0\n",
    "        for col in var_to_plot:\n",
    "            # Count unique values\n",
    "            unique_count = df_S6[col].nunique()\n",
    "            \n",
    "            # Limit the number of plotted categories if there are more than num_values_to_plot\n",
    "            if unique_count > num_values_to_plot:\n",
    "                order = df_S6[col].value_counts().head(num_values_to_plot).index\n",
    "            else:\n",
    "                order = df_S6[col].value_counts().index\n",
    "            \n",
    "            # Create countplot\n",
    "            sns.countplot(\n",
    "                ax = axes[idx],\n",
    "                data = df_S6,\n",
    "                x = col,\n",
    "                hue = y_var,\n",
    "                hue_order = hue_order,\n",
    "                palette = my_palette,\n",
    "                order = order)\n",
    "            axes[idx].tick_params(axis='x', rotation=90, labelsize=my_font_size)\n",
    "            \n",
    "            # Add text box if truncated\n",
    "            if unique_count > num_values_to_plot:\n",
    "                msg = \"There are \" + str(unique_count) + \" different values,\\nbut only \" + str(num_values_to_plot) + \" have been plotted\"\n",
    "                axes[idx].text(\n",
    "                    x = 0.5,\n",
    "                    y = 0.9,\n",
    "                    s = msg,\n",
    "                    transform=axes[idx].transAxes,\n",
    "                    fontsize=my_font_size,\n",
    "                    color='red',\n",
    "                    ha='center',\n",
    "                    va='top',\n",
    "                    bbox=dict(facecolor='grey', alpha=0.25, edgecolor='red'))\n",
    "            \n",
    "            idx += 1\n",
    "\n",
    "        # Turn off unused axes if there are any\n",
    "        for j in range(idx, len(axes)):\n",
    "            axes[j].set_visible(False)\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e523a0",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- all attributes seem to have impact into getting Outcome = 1 (positive), the higher the attribute, the higher the changes to get Outcome = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3540b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 7 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES\n",
    "# -------------------------------\n",
    "print(\"STEP 7 - MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES\")\n",
    "\n",
    "if not make_plots_MULTIVARIANT_ATTRIBUTES:\n",
    "    print(\"‚ö†Ô∏è MULTIVARIANT ANALYSIS - ATTRIBUTES VS ATTRIBUTES is not printed, set make_plots_MULTIVARIANT_ATTRIBUTES = True\")\n",
    "else:\n",
    "    # Copy of previous DataFrame\n",
    "    df_S7 = df_S4.copy()\n",
    "\n",
    "    print(\"\\n üî¢ NUMERIC Attributes VS üî¢ NUMERIC Attributes\")\n",
    "\n",
    "    var_to_plot = numeric_att\n",
    "    num_rows = len(var_to_plot) - 1  # Number of rows (one less than number of variables)\n",
    "\n",
    "    # Create subplots with two stacked plots (regplot + heatmap) per variable pair\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows = num_rows * 2,\n",
    "        ncols = len(var_to_plot) - 1,\n",
    "        figsize=(figWidth_unit * (len(var_to_plot) - 1), figHeight_unit * num_rows),\n",
    "        gridspec_kw={'height_ratios': [4, 1] * num_rows})\n",
    "\n",
    "    # Flatten axes for easy handling\n",
    "    axes = np.array(axes)\n",
    "\n",
    "    # Track subplot usage\n",
    "    for row in range(num_rows):\n",
    "        n_cols = len(var_to_plot) - row - 1  # Decreasing number of columns each row\n",
    "        for col in range(n_cols):\n",
    "\n",
    "            # Top: regplot\n",
    "            sns.regplot(\n",
    "                ax = axes[row * 2, col],\n",
    "                data = df_S7,\n",
    "                x = var_to_plot[row + col + 1],\n",
    "                y = var_to_plot[row],\n",
    "                scatter_kws = {'s': my_font_size, 'alpha': 0.6},\n",
    "                line_kws = {'color': 'red'})\n",
    "            axes[row * 2, col].set_xlabel(var_to_plot[row + col + 1], fontsize=20)\n",
    "            axes[row * 2, col].set_ylabel(var_to_plot[row], fontsize=20)\n",
    "\n",
    "            # Show Y label only for first plot in row\n",
    "            if col == 0:\n",
    "                axes[row * 2, col].set_ylabel(var_to_plot[row], fontsize=my_font_size)\n",
    "            else:\n",
    "                axes[row * 2, col].set_ylabel(\"\")\n",
    "\n",
    "            # Bottom: heatmap (correlation)\n",
    "            sns.heatmap(\n",
    "                ax = axes[row * 2 + 1, col],\n",
    "                data = df_S7[[var_to_plot[row + col + 1], var_to_plot[row]]].corr(),\n",
    "                annot = True,\n",
    "                fmt = \".2f\",\n",
    "                cbar = False,\n",
    "                annot_kws = {\"size\": 20})\n",
    "\n",
    "        # Hide unused subplots on the right for this row\n",
    "        for col in range(n_cols, len(var_to_plot) - 1):\n",
    "            axes[row * 2, col].set_visible(False)\n",
    "            axes[row * 2 + 1, col].set_visible(False)\n",
    "\n",
    "    # Adjust layout and show\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n üè∑Ô∏èüî¢ ALL Attributes VS üè∑Ô∏èüî¢ ALL Attributes\")\n",
    "\n",
    "    # Encode categorical variables using the Series.factorize() method\n",
    "    for col in category_att:\n",
    "        codes, uniques = df_S7[col].factorize()\n",
    "        df_S7[col] = codes  # replace column with integer codes\n",
    "\n",
    "    # CATEGORIC ATTRIBUTES HEATMAP\n",
    "    if len(category_att) > 1:\n",
    "        corr_cat = df_S7[category_att].corr()\n",
    "        fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
    "        plt.title(\"ONLY CATEGORIC ATTRIBUTES\", fontsize=font_size_titles + 2, fontweight=\"bold\")\n",
    "        sns.heatmap(\n",
    "            data=corr_cat,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": my_font_size}\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Not enough categorical attributes for a correlation matrix.\")\n",
    "\n",
    "    # NUMERIC ATTRIBUTES HEATMAP\n",
    "    if len(numeric_att) > 1:\n",
    "        corr_num = df_S7[numeric_att].corr()\n",
    "        fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
    "        plt.title(\"ONLY NUMERIC ATTRIBUTES\", fontsize=font_size_titles + 2, fontweight=\"bold\")\n",
    "        sns.heatmap(\n",
    "            data=corr_num,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": my_font_size}\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Not enough numeric attributes for a correlation matrix.\")\n",
    "\n",
    "    # ALL VARIABLES HEATMAP\n",
    "    corr_matrix = df_S7[numeric_att + category_att].corr()\n",
    "    corr_order = corr_matrix.mean().sort_values(ascending=False).index\n",
    "    corr_matrix = corr_matrix.loc[corr_order, corr_order]\n",
    "\n",
    "    fig = plt.figure(figsize=(2 * figWidth_unit, 2 * figHeight_unit))\n",
    "    plt.title(\"CATEGORIC AND NUMERIC ATTRIBUTES\", fontsize=font_size_titles + 2, fontweight=\"bold\")\n",
    "    sns.heatmap(\n",
    "        data=corr_matrix,\n",
    "        annot=True,\n",
    "        vmin=-1,\n",
    "        vmax=1,\n",
    "        fmt=\".2f\",\n",
    "        annot_kws={\"size\": my_font_size}\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # PAIRPLOT (sorted by correlation order)\n",
    "    fig = plt.figure(figsize=(figWidth_unit, figHeight_unit))\n",
    "    sns.pairplot(data=df_S7[corr_order])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efbae4",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- attributes do not keep a hight correlation between them -> no noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a054f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"STEP 8) MISSING VALUES\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "zero_to_nan = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]                    # List of attributes where zero should be considered missing\n",
    "filling_threshold = 5.0             # [%] If missing perc > filling_threshold ‚Üí fill values, otherwise drop rows\n",
    "grouping_max_unique = 6             # Max number of unique values for a categorical attribute to be usable as keys for grouped median\n",
    "make_missing_values_plots = True    # Make plots?\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "df_S8 = df_S4.copy()\n",
    "\n",
    "# Replace zeros by NaN for selected columns\n",
    "for col in zero_to_nan:\n",
    "    if col in df_S8.columns:\n",
    "        df_S8[col] = df_S8[col].replace(0, np.nan)\n",
    "        print(f\"- ‚ö†Ô∏è Values equal to 0 in '{col}' have been replaced by NaN\")\n",
    "\n",
    "# TARGET VARIABLE\n",
    "missing_y = df_S8[y_var].isnull().sum()\n",
    "\n",
    "if missing_y > 0:\n",
    "    print(f\"- ‚ö†Ô∏è Target variable '{y_var}' contains {missing_y} missing values ‚Üí rows will be dropped.\")\n",
    "    df_S8 = df_S8.dropna(subset=[y_var])\n",
    "else:\n",
    "    print(f\"- ‚úÖ Target variable '{y_var}' has no missing values.\")\n",
    "\n",
    "# Identify categorical variables usable as grouping keys for numeric imputation\n",
    "group_vars = []\n",
    "\n",
    "# Normal categorical attributes\n",
    "for col in category_att:\n",
    "    if df_S8[col].nunique() <= grouping_max_unique:\n",
    "        group_vars.append(col)\n",
    "\n",
    "# Add target as grouping variable if it is CATEGORICAL and has few unique values\n",
    "if y_var_type == \"CATEGORIC\":\n",
    "    if df_S8[y_var].nunique() <= grouping_max_unique:\n",
    "        group_vars.append(y_var)\n",
    "        print(f\"- ‚ÑπÔ∏è Target variable '{y_var}' added to grouping keys for numeric imputation\")\n",
    "\n",
    "# Calculate missing percentages per column\n",
    "missing_pct = (df_S8.isnull().sum() / len(df_S8)) * 100\n",
    "missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_pct) == 0:\n",
    "    print(\"- ‚úÖ DataFrame has no missing values at all (excluding target variable already handled)\")\n",
    "else:\n",
    "    # Process each column with missing values\n",
    "    for col in missing_pct.index:\n",
    "        pct = missing_pct[col]\n",
    "        print(f\"- ‚ö†Ô∏è Column: {col} ‚Üí {pct:.2f}% missing\")\n",
    "\n",
    "        # CASE 1: NUMERIC ATTRIBUTE\n",
    "        if col in numeric_att:\n",
    "\n",
    "            # CASE 1A: grouped median\n",
    "            if pct > filling_threshold and len(group_vars) > 0:\n",
    "                print(f\" - ‚ö†Ô∏è FILLED missing numeric values in {col} using grouped median by {group_vars}...\\n\")\n",
    "\n",
    "                medians = df_S8.groupby(group_vars)[col].median().reset_index()\n",
    "                medians = medians.rename(columns={col: f\"median_{col}\"})\n",
    "\n",
    "                df_S8 = pd.merge(df_S8, medians, on=group_vars, how=\"left\")\n",
    "                df_S8[col] = df_S8[col].fillna(df_S8[f\"median_{col}\"])\n",
    "                df_S8 = df_S8.drop(columns=[f\"median_{col}\"])\n",
    "\n",
    "            # CASE 1B: global median\n",
    "            elif pct > filling_threshold and len(group_vars) == 0:\n",
    "                print(f\" - ‚ö†Ô∏è FILLED missing numeric values in {col} using global median (no grouping columns)...\\n\")\n",
    "                df_S8[col] = df_S8[col].fillna(df_S8[col].median())\n",
    "\n",
    "            # CASE 1C: drop rows\n",
    "            elif pct <= filling_threshold:\n",
    "                print(f\" - ‚ö†Ô∏è DROPPED rows with missing values in {col} ({pct:.2f}% ‚â§ {filling_threshold}%)...\\n\")\n",
    "                df_S8 = df_S8.dropna(subset=[col])\n",
    "\n",
    "        # CASE 2: CATEGORICAL ATTRIBUTE ‚Üí mode imputation\n",
    "        elif col in category_att:\n",
    "\n",
    "            print(f\" - ‚ö†Ô∏è FILLED missing categorical values in {col} using mode (most frequent value)...\\n\")\n",
    "            mode_value = df_S8[col].mode().iloc[0]\n",
    "            df_S8[col] = df_S8[col].fillna(mode_value)\n",
    "\n",
    "        # CASE 3: unsupported\n",
    "        else:\n",
    "            print(f\" - ‚ÑπÔ∏è Column {col} has unsupported type for imputation ‚Äî dropping rows.\\n\")\n",
    "            df_S8 = df_S8.dropna(subset=[col])\n",
    "\n",
    "# Print results\n",
    "print(\"- ‚úÖ Missing values have been handled successfully!\")\n",
    "print(f\" - ‚ÑπÔ∏è Previous df's rows: {len(df_S4)}\")\n",
    "print(f\" - ‚ÑπÔ∏è Current df's rows: {len(df_S8)}\")\n",
    "print(f\" - ‚ÑπÔ∏è Current DataFrame shape: {df_S8.shape}\")\n",
    "print(f\" - ‚ÑπÔ∏è Remaining missing values per column:\\n{df_S8.isnull().sum()}\")\n",
    "\n",
    "if make_missing_values_plots:\n",
    "    # BEFORE vs AFTER missing values handling\n",
    "    print(\"\\nüìä VISUAL CHECK - BEFORE vs AFTER missing values handling\")\n",
    "\n",
    "    df_S8_before = df_S4.copy()   # Before missing-value handling\n",
    "    df_S8_after = df_S8.copy()    # After missing-value handling\n",
    "\n",
    "    if not numeric_att:\n",
    "        print(\"   This type of plot is non applicable for this case, because there are not NUMERIC variables in the DataFrame\")\n",
    "\n",
    "    else:\n",
    "        var_to_plot = numeric_att.copy()\n",
    "        if y_var_type == \"NUMERIC\" and y_var not in var_to_plot:\n",
    "            var_to_plot.insert(0, y_var)\n",
    "\n",
    "        num_rows = len(var_to_plot)\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows * 2,\n",
    "            ncols = 2,\n",
    "            figsize = (figWidth_unit * 2, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'height_ratios': [4, 0.5] * num_rows})\n",
    "\n",
    "        for i, colname in enumerate(var_to_plot):\n",
    "\n",
    "            # Row indices for histogram and boxplot of this variable\n",
    "            hist_row  = i * 2\n",
    "            box_row   = i * 2 + 1\n",
    "\n",
    "            # Common bins (syncronize BEFORE and AFTER)\n",
    "            xmin = min(df_S8_before[colname].min(), df_S8_after[colname].min())\n",
    "            xmax = max(df_S8_before[colname].max(), df_S8_after[colname].max())\n",
    "            common_bins = np.linspace(xmin, xmax, num_bins + 1)\n",
    "\n",
    "            # ================\n",
    "            # BEFORE PLOTS\n",
    "            # ================\n",
    "            before_hist_ax = axes[hist_row, 0]\n",
    "            before_box_ax  = axes[box_row, 0]\n",
    "\n",
    "            sns.histplot(\n",
    "                ax = before_hist_ax,\n",
    "                data = df_S8_before,\n",
    "                x = colname,\n",
    "                bins = num_bins,\n",
    "                color = \"gray\",\n",
    "                alpha = 0.35)\n",
    "            before_hist_ax.set_title(colname + \" - BEFORE\")\n",
    "            before_hist_ax.set_xlabel(\"\")\n",
    "\n",
    "            sns.boxplot(\n",
    "                ax = before_box_ax,\n",
    "                data = df_S8_before,\n",
    "                x = colname,\n",
    "                color = \"lightgray\")\n",
    "            before_box_ax.set_xlabel(\"\")\n",
    "\n",
    "            # Store BEFORE axis limits\n",
    "            xlim_hist_before = before_hist_ax.get_xlim()\n",
    "            ylim_hist_before = before_hist_ax.get_ylim()\n",
    "            xlim_box_before  = before_box_ax.get_xlim()\n",
    "\n",
    "            # ================\n",
    "            # AFTER PLOTS\n",
    "            # ================\n",
    "            after_hist_ax = axes[hist_row, 1]\n",
    "            after_box_ax  = axes[box_row, 1]\n",
    "\n",
    "            sns.histplot(\n",
    "                ax = after_hist_ax,\n",
    "                data = df_S8_after,\n",
    "                x = colname,\n",
    "                bins = common_bins)\n",
    "            after_hist_ax.set_title(colname + \" - AFTER\")\n",
    "            after_hist_ax.set_xlabel(\"\")\n",
    "\n",
    "            sns.boxplot(\n",
    "                ax = after_box_ax,\n",
    "                data = df_S8_after,\n",
    "                x = colname)\n",
    "            after_box_ax.set_xlabel(\"\")\n",
    "\n",
    "            # Syncronize axes limits\n",
    "            after_hist_ax.set_xlim(xlim_hist_before)\n",
    "            after_hist_ax.set_ylim(ylim_hist_before)\n",
    "            after_box_ax.set_xlim(xlim_box_before)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69fee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 9) CLEAN OUTLIERS\n",
    "# -------------------------------\n",
    "print(\"STEP 9) CLEAN OUTLIERS\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "low_outliers_threshold = 1.0        # [%] Max percentage of lower outliers allowed to remove\n",
    "up_outliers_threshold = 1.0         # [% ]Max percentage of upper outliers allowed to remove\n",
    "removal_type = \"EXTREME OUTLIERS\"   # Removal logic type (NORMAL or EXTREME outliers)\n",
    "make_outliers_plots = True          # Make plots?\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_S9=df_S8.copy()\n",
    "\n",
    "# Print info\n",
    "display(df_S9.describe())\n",
    "\n",
    "# Outliers detection is applied over numeric attributes\n",
    "cols = []\n",
    "for col in numeric_att:\n",
    "    if col in df_S9.columns.to_list():\n",
    "        cols.append(col)\n",
    "\n",
    "# Initialize containers\n",
    "lower_limits = []\n",
    "upper_limits = []\n",
    "n_outliers_lower = []\n",
    "n_outliers_upper = []\n",
    "pct_outliers_lower = []\n",
    "pct_outliers_upper = []\n",
    "extreme_lower_limits = []\n",
    "extreme_upper_limits = []\n",
    "n_extreme_outliers_lower = []\n",
    "n_extreme_outliers_upper = []\n",
    "pct_extreme_outliers_lower = []\n",
    "pct_extreme_outliers_upper = []\n",
    "\n",
    "len(df_S9.index)\n",
    "\n",
    "for col in cols:\n",
    "    Q1 = df_S9[col].quantile(0.25)\n",
    "    Q3 = df_S9[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Calulate limits\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    extreme_lower = Q1 - 3 * IQR\n",
    "    extreme_upper = Q3 + 3 * IQR\n",
    "\n",
    "    # Count num of outliers\n",
    "    n_low = (df_S9[col] < lower).sum()\n",
    "    n_high = (df_S9[col] > upper).sum()\n",
    "    n_extreme_low = (df_S9[col] < extreme_lower).sum()\n",
    "    n_extreme_high = (df_S9[col] > extreme_upper).sum()\n",
    "\n",
    "    # Percentages of outliers\n",
    "    pct_low = (n_low / len(df_S9.index)) * 100\n",
    "    pct_high = (n_high / len(df_S9.index)) * 100\n",
    "    pct_extreme_low = (n_extreme_low / len(df_S9.index)) * 100\n",
    "    pct_extreme_high = (n_extreme_high / len(df_S9.index)) * 100\n",
    "\n",
    "    # Save limits\n",
    "    lower_limits.append(lower)\n",
    "    upper_limits.append(upper)\n",
    "    extreme_lower_limits.append(extreme_lower)\n",
    "    extreme_upper_limits.append(extreme_upper)\n",
    "\n",
    "    # Save num of outliers\n",
    "    n_outliers_lower.append(n_low)\n",
    "    n_outliers_upper.append(n_high)\n",
    "    n_extreme_outliers_lower.append(n_extreme_low)\n",
    "    n_extreme_outliers_upper.append(n_extreme_high)\n",
    "\n",
    "    # Save percentages of outliers\n",
    "    pct_outliers_lower.append(pct_low)\n",
    "    pct_outliers_upper.append(pct_high)\n",
    "    pct_extreme_outliers_lower.append(pct_extreme_low)\n",
    "    pct_extreme_outliers_upper.append(pct_extreme_high)\n",
    "\n",
    "\n",
    "# Build DataFrame with all results\n",
    "df_limits = pd.DataFrame(\n",
    "    [\n",
    "        lower_limits,\n",
    "        upper_limits,\n",
    "        n_outliers_lower,\n",
    "        n_outliers_upper,\n",
    "        pct_outliers_lower,\n",
    "        pct_outliers_upper\n",
    "    ],\n",
    "    columns=cols,\n",
    "    index=[\n",
    "        \"lower_limit\",\n",
    "        \"upper_limit\",\n",
    "        \"n_outliers_lower\",\n",
    "        \"n_outliers_upper\",\n",
    "        \"pct_outliers_lower\",\n",
    "        \"pct_outliers_upper\"\n",
    "    ]\n",
    ")\n",
    "df_extreme_limits = pd.DataFrame(\n",
    "    [\n",
    "        extreme_lower_limits,\n",
    "        extreme_upper_limits,\n",
    "        n_extreme_outliers_lower,\n",
    "        n_extreme_outliers_upper,\n",
    "        pct_extreme_outliers_lower,\n",
    "        pct_extreme_outliers_upper\n",
    "    ],\n",
    "    columns=cols,\n",
    "    index=[\n",
    "        \"extreme_lower_limit\",\n",
    "        \"extreme_upper_limit\",\n",
    "        \"n_extreme_outliers_lower\",\n",
    "        \"n_extreme_outliers_upper\",\n",
    "        \"pct_extreme_outliers_lower\",\n",
    "        \"pct_extreme_outliers_upper\"\n",
    "    ]\n",
    ")\n",
    "# Display results\n",
    "display(df_limits)\n",
    "display(df_extreme_limits)\n",
    "\n",
    "# Outliers detection is applied over numeric attributes\n",
    "cols = []\n",
    "for col in numeric_att:\n",
    "    if col in df_S9.columns.to_list():\n",
    "        cols.append(col)\n",
    "\n",
    "if removal_type == \"NORMAL OUTLIERS\":\n",
    "    # Loop through each column and apply filtering rules\n",
    "    for col in cols:\n",
    "        low_limit = df_limits.loc[\"lower_limit\", col]\n",
    "        high_limit = df_limits.loc[\"upper_limit\", col]\n",
    "        pct_low = df_limits.loc[\"pct_outliers_lower\", col]\n",
    "        pct_high = df_limits.loc[\"pct_outliers_upper\", col]\n",
    "        \n",
    "        # Remove low outliers if below threshold\n",
    "        print(f\"\\n- ATTRIBUTE {col}:\")\n",
    "        if pct_low == 0:\n",
    "            print(f\" - ‚ÑπÔ∏è None lower outliers detected\")\n",
    "        elif pct_low <= low_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] >= low_limit]\n",
    "            print(f\" - ‚ö†Ô∏è REMOVED lower outliers ({pct_low:.2f}% <= {low_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ‚úÖ KEPT lower outliers ({pct_low:.2f}% > {low_outliers_threshold}%)\")\n",
    "        \n",
    "        # Remove high outliers if below threshold\n",
    "        if pct_high == 0:\n",
    "            print(f\" - ‚ÑπÔ∏è None upper outliers detected\")\n",
    "        elif pct_high <= up_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] <= high_limit]\n",
    "            print(f\" - ‚ö†Ô∏è REMOVED upper outliers ({pct_high:.2f}% <= {up_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ‚úÖ KEPT upper outliers ({pct_high:.2f}% > {up_outliers_threshold}%)\")\n",
    "\n",
    "elif removal_type == \"EXTREME OUTLIERS\":\n",
    "    # Loop through each column and apply filtering rules\n",
    "    for col in cols:\n",
    "        low_limit = df_extreme_limits.loc[\"extreme_lower_limit\", col]\n",
    "        high_limit = df_extreme_limits.loc[\"extreme_upper_limit\", col]\n",
    "        pct_low = df_extreme_limits.loc[\"pct_extreme_outliers_lower\", col]\n",
    "        pct_high = df_extreme_limits.loc[\"pct_extreme_outliers_upper\", col]\n",
    "        \n",
    "        # Remove low outliers if below threshold\n",
    "        print(f\"\\n- ATTRIBUTE {col}:\")\n",
    "        if pct_low == 0:\n",
    "            print(f\" - ‚ÑπÔ∏è None extreme lower outliers detected\")\n",
    "        elif pct_low <= low_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] >= low_limit]\n",
    "            print(f\" - ‚ö†Ô∏è REMOVED extreme lower outliers ({pct_low:.2f}% <= {low_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ‚úÖ KEPT extreme lower outliers ({pct_low:.2f}% > {low_outliers_threshold}%)\")\n",
    "        \n",
    "        # Remove high outliers if below threshold\n",
    "        if pct_high == 0:\n",
    "            print(f\" - ‚ÑπÔ∏è None extreme upper outliers detected\")\n",
    "        elif pct_high <= up_outliers_threshold:\n",
    "            df_S9 = df_S9[df_S9[col] <= high_limit]\n",
    "            print(f\" - ‚ö†Ô∏è REMOVED extreme upper outliers ({pct_high:.2f}% <= {up_outliers_threshold}%)\")\n",
    "        else:\n",
    "            print(f\" - ‚úÖ KEPT extreme upper outliers ({pct_high:.2f}% > {up_outliers_threshold}%)\")\n",
    "\n",
    "# Print results\n",
    "print(\"- ‚úÖ Outliers have been handled successfully!\")\n",
    "print(f\" - ‚ÑπÔ∏è Previous df's rows: {len(df_S8)}\")\n",
    "print(f\" - ‚ÑπÔ∏è Current df's rows: {len(df_S9)}\")\n",
    "print(f\" - ‚ÑπÔ∏è Current DataFrame shape: {df_S9.shape}\")\n",
    "display(df_S9.describe())\n",
    "\n",
    "if make_outliers_plots:\n",
    "    # BEFORE vs AFTER Outliers handling\n",
    "    print(\"\\nüìä VISUAL CHECK - BEFORE vs AFTER outliers handling\")\n",
    "\n",
    "    df_S9_before = df_S8.copy()   # Before missing-value handling\n",
    "    df_S9_after = df_S9.copy()    # After missing-value handling\n",
    "\n",
    "    if not numeric_att:\n",
    "        print(\"   This type of plot is non applicable for this case, because there are not NUMERIC variables in the DataFrame\")\n",
    "\n",
    "    else:\n",
    "        var_to_plot = numeric_att\n",
    "        num_rows = len(var_to_plot)\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows = num_rows * 2,\n",
    "            ncols = 2,\n",
    "            figsize = (figWidth_unit * 2, figHeight_unit * num_rows),\n",
    "            gridspec_kw={'height_ratios': [4, 0.5] * num_rows})\n",
    "\n",
    "        for i, colname in enumerate(var_to_plot):\n",
    "\n",
    "            # Row indices for histogram and boxplot of this variable\n",
    "            hist_row  = i * 2\n",
    "            box_row   = i * 2 + 1\n",
    "\n",
    "            # Set common bins (syncronize BEFORE and AFTER)\n",
    "            xmin = min(df_S9_before[colname].min(), df_S9_after[colname].min())\n",
    "            xmax = max(df_S9_before[colname].max(), df_S9_after[colname].max())\n",
    "            common_bins = np.linspace(xmin, xmax, num_bins + 1)\n",
    "\n",
    "            # Set colored area limits\n",
    "            normal_low = df_limits.loc[\"lower_limit\", colname]\n",
    "            normal_up  = df_limits.loc[\"upper_limit\", colname]\n",
    "            extreme_low = df_extreme_limits.loc[\"extreme_lower_limit\", colname]\n",
    "            extreme_up  = df_extreme_limits.loc[\"extreme_upper_limit\", colname]\n",
    "\n",
    "            # ================\n",
    "            # BEFORE PLOTS\n",
    "            # ================\n",
    "            before_hist_ax = axes[hist_row, 0]\n",
    "            before_box_ax  = axes[box_row, 0]\n",
    "\n",
    "            sns.histplot(\n",
    "                ax = before_hist_ax,\n",
    "                data = df_S9_before,\n",
    "                x = colname,\n",
    "                bins = num_bins,\n",
    "                color = \"gray\",\n",
    "                alpha = 0.35)\n",
    "            before_hist_ax.set_title(colname + \" - BEFORE\")\n",
    "            before_hist_ax.set_xlabel(\"\")\n",
    "\n",
    "            sns.boxplot(\n",
    "                ax = before_box_ax,\n",
    "                data = df_S9_before,\n",
    "                x = colname,\n",
    "                color = \"lightgray\")\n",
    "            before_box_ax.set_xlabel(\"\")\n",
    "\n",
    "            # Outlier count\n",
    "            pct_low_normal  = df_limits.loc[\"pct_outliers_lower\", colname]\n",
    "            pct_high_normal = df_limits.loc[\"pct_outliers_upper\", colname]\n",
    "            pct_low_extreme  = df_extreme_limits.loc[\"pct_extreme_outliers_lower\", colname]\n",
    "            pct_high_extreme = df_extreme_limits.loc[\"pct_extreme_outliers_upper\", colname]\n",
    "\n",
    "            # NORMAL Outliers\n",
    "            if pct_low_normal > 0:\n",
    "                before_hist_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
    "                before_box_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
    "            if pct_high_normal > 0:\n",
    "                before_hist_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
    "                before_box_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
    "\n",
    "            # EXTREME Outliers\n",
    "            if pct_low_extreme > 0:\n",
    "                before_hist_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
    "                before_box_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
    "            if pct_high_extreme > 0:\n",
    "                before_hist_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
    "                before_box_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
    "\n",
    "            # Store BEFORE limits\n",
    "            xlim_hist_before = before_hist_ax.get_xlim()\n",
    "            ylim_hist_before = before_hist_ax.get_ylim()\n",
    "            xlim_box_before  = before_box_ax.get_xlim()\n",
    "\n",
    "            # ================\n",
    "            # AFTER PLOTS\n",
    "            # ================\n",
    "            after_hist_ax = axes[hist_row, 1]\n",
    "            after_box_ax  = axes[box_row, 1]\n",
    "\n",
    "            sns.histplot(\n",
    "                ax = after_hist_ax,\n",
    "                data = df_S9_after,\n",
    "                x = colname,\n",
    "                bins = common_bins)\n",
    "            after_hist_ax.set_title(colname + \" - AFTER\")\n",
    "            after_hist_ax.set_xlabel(\"\")\n",
    "\n",
    "            sns.boxplot(\n",
    "                ax = after_box_ax,\n",
    "                data = df_S9_after,\n",
    "                x = colname)\n",
    "            after_box_ax.set_xlabel(\"\")\n",
    "\n",
    "            # Check if outliers are still present in AFTER\n",
    "            normal_low_present  = (df_S9_after[colname] < normal_low).any()\n",
    "            normal_up_present   = (df_S9_after[colname] > normal_up).any()\n",
    "            extreme_low_present = (df_S9_after[colname] < extreme_low).any()\n",
    "            extreme_up_present  = (df_S9_after[colname] > extreme_up).any()\n",
    "\n",
    "            # NORMAL Outliers\n",
    "            if normal_low_present:\n",
    "                after_hist_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
    "                after_box_ax.axvspan(normal_low, extreme_low, color=\"orange\", alpha=0.22)\n",
    "            if normal_up_present:\n",
    "                after_hist_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
    "                after_box_ax.axvspan(extreme_up, normal_up, color=\"orange\", alpha=0.22)\n",
    "\n",
    "            # EXTREME Outliers\n",
    "            if extreme_low_present:\n",
    "                after_hist_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
    "                after_box_ax.axvspan(xmin, extreme_low, color=\"red\", alpha=0.22)\n",
    "            if extreme_up_present:\n",
    "                after_hist_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
    "                after_box_ax.axvspan(extreme_up, xmax, color=\"red\", alpha=0.22)\n",
    "\n",
    "            # Legends\n",
    "            before_hist_ax.legend(\n",
    "                handles=[\n",
    "                    plt.Rectangle((0,0),1,1, facecolor='orange', alpha=0.45, label=\"NORMAL OUTLIERS\"),\n",
    "                    plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.45, label=\"EXTREME OUTLIERS\")],\n",
    "                loc=\"upper right\")\n",
    "            after_hist_ax.legend(\n",
    "                handles=[\n",
    "                    plt.Rectangle((0,0),1,1, facecolor='orange', alpha=0.45, label=\"NORMAL OUTLIERS\"),\n",
    "                    plt.Rectangle((0,0),1,1, facecolor='red', alpha=0.45, label=\"EXTREME OUTLIERS\")],\n",
    "                loc=\"upper right\")\n",
    "\n",
    "            # Syncronize axes limits\n",
    "            after_hist_ax.set_xlim(xlim_hist_before)\n",
    "            after_hist_ax.set_ylim(ylim_hist_before)\n",
    "            after_box_ax.set_xlim(xlim_box_before)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5119c",
   "metadata": {},
   "source": [
    "CONCLUSIONS:\n",
    "- I have stablished a \"cut\" based on EXTREME outliers\n",
    "- The \"cut\" would be for a maximum of 1 %, higher that than that, EXTREME outliers will be kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48119caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 10) REMOVE NOISY ATTRIBUTES\n",
    "# -------------------------------\n",
    "print(\"STEP 10) REMOVE NOISY ATTRIBUTES\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "corr_threshold = 0.9 # Correlation level considered as \"too high\"\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "df_S10 = df_S9.copy()\n",
    "\n",
    "#  NUMERIC ATTRIBUTES (Pearson correlation)\n",
    "corr_matrix = df_S10[numeric_att].corr().abs()\n",
    "to_drop = set()\n",
    "\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if corr_matrix.iloc[i, j] > corr_threshold:\n",
    "            col_i = corr_matrix.columns[i]\n",
    "            col_j = corr_matrix.columns[j]\n",
    "            if col_i not in to_drop:\n",
    "                to_drop.add(col_i)\n",
    "\n",
    "if to_drop:\n",
    "    df_S10 = df_S10.drop(columns=list(to_drop), axis=1)\n",
    "    print(f\"- ‚ö†Ô∏è High NUMERIC attributes correlation detected (Pearson Corr. > {corr_threshold}). Dropped:\")\n",
    "    for col in to_drop:\n",
    "        print(f\"   ‚Ä¢ {col}\")\n",
    "else:\n",
    "    print(f\"- ‚úÖ No NUMERIC attributes exceeded {corr_threshold} Pearson Correlation\")\n",
    "\n",
    "#  CATEGORICAL ATTRIBUTES (Cram√©r's V)\n",
    "def cramers_v(x, y): \n",
    "    # Step 1: confusion matrix\n",
    "    confusion_matrix = pd.crosstab(x, y)\n",
    "    # Step 2: chi-square statistic\n",
    "    chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "    # Step 3: phi-squared\n",
    "    total_samples = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2 / total_samples\n",
    "    # Shape of confusion matrix\n",
    "    r, k = confusion_matrix.shape\n",
    "    num_rows = confusion_matrix.shape[0]\n",
    "    num_cols = confusion_matrix.shape[1]\n",
    "    # Step 4: bias correction (recommended formula)\n",
    "    correction = ((num_cols - 1) * (num_rows - 1)) / (total_samples - 1)\n",
    "    phi2_corrected = max(0, phi2 - correction)\n",
    "    # Corrected dimensions\n",
    "    rows_corrected = num_rows - ((num_rows - 1) ** 2) / (total_samples - 1)\n",
    "    cols_corrected = num_cols - ((num_cols - 1) ** 2) / (total_samples - 1)\n",
    "    # Step 5: compute Cram√©r's V\n",
    "    denominator = min(rows_corrected - 1, cols_corrected - 1)\n",
    "    if denominator <= 0:\n",
    "        return 0  # avoid division by zero for degenerate tables\n",
    "    cramers_v_value = np.sqrt(phi2_corrected / denominator)\n",
    "    return cramers_v_value\n",
    "\n",
    "to_drop_cat = set()\n",
    "\n",
    "if len(category_att) > 1:\n",
    "    for i in range(len(category_att)):\n",
    "        for j in range(i):\n",
    "            v = cramers_v(df_S10[category_att[i]], df_S10[category_att[j]])\n",
    "            if v > corr_threshold:\n",
    "                col_i = category_att[i]\n",
    "                col_j = category_att[j]\n",
    "                if col_i not in to_drop_cat:\n",
    "                    to_drop_cat.add(col_i)\n",
    "\n",
    "if to_drop_cat:\n",
    "    df_S10 = df_S10.drop(columns=list(to_drop_cat), axis=1)\n",
    "    print(f\"- ‚ö†Ô∏è High CATEGORICAL attributes association detected (Cramer‚Äôs V > {corr_threshold}). Dropped:\")\n",
    "    for col in to_drop_cat:\n",
    "        print(f\"   ‚Ä¢ {col}\")\n",
    "else:\n",
    "    print(f\"- ‚úÖ No CATEGORICAL attributes exceeded {corr_threshold} Cramer‚Äôs V\")\n",
    "\n",
    "# Update numeric_att\n",
    "updated_numeric = []\n",
    "for col in numeric_att:\n",
    "    if col in df_S10.columns:\n",
    "        updated_numeric.append(col)\n",
    "numeric_att = updated_numeric\n",
    "\n",
    "# Update category_att\n",
    "updated_category = []\n",
    "for col in category_att:\n",
    "    if col in df_S10.columns:\n",
    "        updated_category.append(col)\n",
    "category_att = updated_category\n",
    "\n",
    "# Update binary_att\n",
    "updated_binary = []\n",
    "for col in binary_att:\n",
    "    if col in df_S10.columns:\n",
    "        updated_binary.append(col)\n",
    "binary_att = updated_binary\n",
    "\n",
    "# Update multiclass_att\n",
    "updated_multiclass = []\n",
    "for col in multiclass_att:\n",
    "    if col in df_S10.columns:\n",
    "        updated_multiclass.append(col)\n",
    "multiclass_att = updated_multiclass\n",
    "\n",
    "#  Print results\n",
    "print(f\"- ‚ÑπÔ∏è Previous df's columns: {len(df_S9.columns)}\")\n",
    "print(f\"- ‚ÑπÔ∏è Cleaned df's  columns: {len(df_S10.columns)}\")\n",
    "print(f\"- ‚ÑπÔ∏è Final DataFrame shape: {df_S10.shape}\")\n",
    "display(df_S10.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8495fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 11) SPLIT\n",
    "# -------------------------------\n",
    "print(\"STEP 11) SPLIT\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "my_test_size = 0.2\n",
    "random_seed = 42\n",
    "\n",
    "# Copy  of previous DataFrame\n",
    "df_SPLIT = df_S10.copy()\n",
    "\n",
    "# Separate attributes from target variable\n",
    "X = df_SPLIT.drop(labels = y_var, axis = 1)\n",
    "y = df_SPLIT[y_var]\n",
    "\n",
    "# Make split between Train and Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = my_test_size, random_state = random_seed)\n",
    "\n",
    "print(\"- ‚ÑπÔ∏è Shape of DataFrames after SPLIT:\")\n",
    "print(\" - X_train:\",X_train.shape)\n",
    "print(\" - X_test:\",X_test.shape)\n",
    "print(\" - y_train:\",y_train.shape)\n",
    "print(\" - y_test:\",y_test.shape)\n",
    "\n",
    "print(\"\\n- ‚ÑπÔ∏è Content of DataFrames after SPLIT:\")\n",
    "print(\" - X_train:\\n\",X_train.head(5))\n",
    "print(\" - X_test:\\n\",X_test.head(5))\n",
    "print(\" - y_train:\\n\",y_train.head(5))\n",
    "print(\" - y_test:\\n\",y_test.head(5))\n",
    "\n",
    "print(\"\\n- ‚ÑπÔ∏è Info of DataFrames after SPLIT:\")\n",
    "print(\" - X_train:\",X_train.info())\n",
    "print(\" - X_test:\",X_test.info())\n",
    "print(\" - y_train:\",y_train.info())\n",
    "print(\" - y_test:\",y_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de51baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 12) SCALLING\n",
    "# -------------------------------\n",
    "print(\"STEP 12) SCALLING\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "make_scalling = False\n",
    "scaler_dic={# Instance scaler for each numeric attribute - change manually StandardScaler() or MinMaxScaler()\n",
    "}\n",
    "for col in X_train.columns:\n",
    "    if col in numeric_att:\n",
    "        scaler_dic[col] = StandardScaler()\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "X_train_es = X_train.copy()\n",
    "X_test_es = X_test.copy()\n",
    "\n",
    "if not numeric_att:\n",
    "    X_train_es = X_train_es[numeric_att]\n",
    "    X_test_es = X_test_es[numeric_att]\n",
    "    print(\"  ‚ö†Ô∏è SCALLING is non applicable for this case, because there are not NUMERIC attributes in the DataFrame\")\n",
    "elif not make_scalling:\n",
    "    X_train_es = X_train_es[numeric_att]\n",
    "    X_test_es = X_test_es[numeric_att]\n",
    "    print(\"  ‚ö†Ô∏è SCALLING is is not carried out, set make_scalling = True\")\n",
    "    display(X_train_es.head())\n",
    "else:\n",
    "\n",
    "    # Fit scalers ONLY on train data\n",
    "    for col in numeric_att:\n",
    "        scaler_dic[col].fit(X_train_es[[col]])\n",
    "    print(\"- ‚úÖ All Scalers have been trained successfully\")\n",
    "\n",
    "    # Apply scalers and create scaled columns\n",
    "    scaled_cols = []\n",
    "\n",
    "    for col in numeric_att:\n",
    "        # Detect scaler type\n",
    "        scaler_name = scaler_dic[col].__class__.__name__\n",
    "        # Set suffix\n",
    "        if scaler_name == \"StandardScaler\":\n",
    "            suffix = \"_SS\"\n",
    "        elif scaler_name == \"MinMaxScaler\":\n",
    "            suffix = \"_MM\"\n",
    "        else:\n",
    "            suffix = \"_Scaled\"\n",
    "\n",
    "        # Transform\n",
    "        X_train_es[col + suffix] = scaler_dic[col].transform(X_train_es[[col]])\n",
    "        X_test_es[col + suffix]  = scaler_dic[col].transform(X_test_es[[col]])\n",
    "\n",
    "        scaled_cols.append(col + suffix)\n",
    "        print(f\"- ‚úÖ Train/Test scaled for: {col} using {scaler_name} ‚Üí new column: {col + suffix}\")\n",
    "\n",
    "    # Keep only scaled columns\n",
    "    X_train_es = X_train_es[scaled_cols]\n",
    "    X_test_es  = X_test_es[scaled_cols]\n",
    "\n",
    "    print(\"- ‚úÖ Final scaled datasets created successfully!\")\n",
    "    display(X_train_es.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49337c58",
   "metadata": {},
   "source": [
    "CONCLUSION:\n",
    "- I do not make scalling beause Random Forest is an algorithm that does not need scalled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866305f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 13) ENCODING\n",
    "# -------------------------------\n",
    "print(\"STEP 13) ENCODING\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "make_encoding = True\n",
    "\n",
    "# Copy of previous DataFrame\n",
    "X_train_en = X_train.copy()\n",
    "X_test_en = X_test.copy()\n",
    "\n",
    "if not category_att:\n",
    "    X_train_en = X_train_en[category_att]\n",
    "    X_test_en = X_test_en[category_att]\n",
    "    print(\"  ‚ö†Ô∏è ENCODING is non applicable for this case, because there are not CATEGORIC attributes in the DataFrame\")\n",
    "elif not make_encoding:\n",
    "    X_train_en = X_train_en[category_att]\n",
    "    X_test_en = X_test_en[category_att]\n",
    "    print(\"  ‚ö†Ô∏è ENCODING is is not carried out, set make_encoding = True\")\n",
    "    display(X_train_en.head())\n",
    "else:\n",
    "    # List of columns\n",
    "    columns = X_train_en.columns.tolist()\n",
    "\n",
    "    # Create encoder instance for each categorical attribute\n",
    "    encoder_dic = {}\n",
    "    for col in category_att:\n",
    "        if col in binary_att:\n",
    "            encoder_dic[col] = LabelEncoder()\n",
    "            print(f\"- ‚úÖ Encoder instanced successfully for {col}: LabelEncoder()\")\n",
    "\n",
    "        elif col in multiclass_att:\n",
    "            encoder_dic[col] = OneHotEncoder(sparse_output=False)\n",
    "            print(f\"- ‚úÖ Encoder instanced successfully for {col}: OneHotEncoder()\")\n",
    "\n",
    "    # Train encoders with TRAIN data only\n",
    "    for col in category_att:\n",
    "        encoder = encoder_dic[col]\n",
    "        if isinstance(encoder, LabelEncoder):\n",
    "            encoder.fit(X_train_en[col])        # LabelEncoder needs 1D\n",
    "            print(f\"- ‚úÖ Encoder trained successfully with {col} from Train: LabelEncoder()\")\n",
    "\n",
    "        elif isinstance(encoder, OneHotEncoder):\n",
    "            encoder.fit(X_train_en[[col]])      # OHE needs 2D\n",
    "            print(f\"- ‚úÖ Encoder trained successfully with {col} from Train: OneHotEncoder()\")\n",
    "\n",
    "    # Apply encoders to TRAIN + TEST\n",
    "    for col in category_att:\n",
    "        encoder = encoder_dic[col]\n",
    "        if isinstance(encoder, LabelEncoder):\n",
    "            X_train_en[col + \"_LE\"] = encoder.transform(X_train_en[col])\n",
    "            X_test_en[col + \"_LE\"] = encoder.transform(X_test_en[col])\n",
    "            print(f\"- ‚úÖ Train/Test encoded for: {col} using LabelEncoder()\")\n",
    "\n",
    "        elif isinstance(encoder, OneHotEncoder):\n",
    "\n",
    "            # Transform train and test\n",
    "            train_encoded = encoder.transform(X_train_en[[col]])\n",
    "            test_encoded = encoder.transform(X_test_en[[col]])\n",
    "\n",
    "            # New names\n",
    "            ohe_colnames = encoder.get_feature_names_out([col])\n",
    "            ohe_colnames = [name + \"_OHE\" for name in ohe_colnames]\n",
    "\n",
    "            # Convert to DataFrames\n",
    "            train_ohe_df = pd.DataFrame(train_encoded, index=X_train_en.index, columns=ohe_colnames)\n",
    "            test_ohe_df = pd.DataFrame(test_encoded, index=X_test_en.index, columns=ohe_colnames)\n",
    "\n",
    "            # Concatenate new cols\n",
    "            X_train_en = pd.concat([X_train_en, train_ohe_df], axis=1)\n",
    "            X_test_en = pd.concat([X_test_en, test_ohe_df], axis=1)\n",
    "\n",
    "            print(f\"- ‚úÖ Train/Test encoded for: {col} using OneHotEncoder()\")\n",
    "\n",
    "    # Keep only encoded columns\n",
    "    encoded_cols = []\n",
    "    for col in category_att:\n",
    "        encoder = encoder_dic[col]\n",
    "        if isinstance(encoder, LabelEncoder):\n",
    "            encoded_cols.append(col + \"_LE\")\n",
    "        elif isinstance(encoder, OneHotEncoder):\n",
    "            ohe_colnames = encoder.get_feature_names_out([col])\n",
    "            for name in ohe_colnames:\n",
    "                encoded_cols.append(name + \"_OHE\")\n",
    "\n",
    "    X_train_en = X_train_en[encoded_cols]\n",
    "    X_test_en = X_test_en[encoded_cols]\n",
    "\n",
    "    print(\"- ‚úÖ Final encoded datasets created successfully\")\n",
    "    display(X_train_en.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 14) FEATURE SELECTION\n",
    "# -------------------------------\n",
    "print(\"STEP 14) FEATURE SELECTION\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "feature_keeping_threshold = 100 # [%] Percentaje of features to keep (SelectKBest) \n",
    "\n",
    "# Concatenate NUMERIC_var_scaled with CATEGORY_var_encoded\n",
    "X_train_assembled = pd.concat([X_train_es, X_train_en], axis=1)\n",
    "X_test_assembled = pd.concat([X_test_es, X_test_en], axis=1)\n",
    "\n",
    "# Instance selector\n",
    "num_features_to_keep = round(feature_keeping_threshold/100 * len(X_train_assembled.columns))\n",
    "selection_model = SelectKBest(score_func = f_classif, k = num_features_to_keep)\n",
    "print(\"- ‚úÖ Selector have been instanced successfully to keep \" + str(num_features_to_keep) + \" features\")\n",
    "\n",
    "# Train selector with ONLY train data (y_train must be included because this is SUPERVISED selector)\n",
    "selection_model.fit(X_train_assembled, y_train)\n",
    "print(\"- ‚úÖ Selector have been trained successfully\")\n",
    "\n",
    "# Drop non-selected features\n",
    "keeping_mask = selection_model.get_support()\n",
    "X_train_assembled = pd.DataFrame(selection_model.transform(X_train_assembled), columns = X_train_assembled.columns.values[keeping_mask])\n",
    "X_test_assembled = pd.DataFrame(selection_model.transform(X_test_assembled), columns = X_test_assembled.columns.values[keeping_mask])\n",
    "\n",
    "# Print results\n",
    "print(\"\\n üßÆ X_train_assembled\", X_train_assembled.shape)\n",
    "display(X_train_assembled.head())\n",
    "print(\"\\n üßÆ X_test_assembled\", X_test_assembled.shape)\n",
    "display(X_test_assembled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ae6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 15) SAVE PROCESSED DATA\n",
    "# -------------------------------\n",
    "print(\"STEP 15) SAVE PROCESSED DATA\")\n",
    "\n",
    "# ||||||||||||||||||\n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "output_path = \"../data/processed/\"\n",
    "\n",
    "# Set from previous DataFrame\n",
    "X_train_final = X_train_assembled\n",
    "X_test_final = X_test_assembled\n",
    "y_train_final = y_train\n",
    "y_test_final = y_test\n",
    "\n",
    "# Get revision number - Returns the next free integer revision based on existing files\n",
    "import os\n",
    "def get_revision_number(base_path, base_name):\n",
    "    rev = 0\n",
    "    while True:\n",
    "        full_path = os.path.join(base_path, base_name + \"_\" + str(rev) + \".csv\")\n",
    "        if not os.path.exists(full_path):\n",
    "            return rev\n",
    "        rev += 1\n",
    "\n",
    "# Build filenames WITH revision number\n",
    "rev_number = get_revision_number(output_path, \"X_train_final\")\n",
    "suffix = \"_\" + str(rev_number)\n",
    "\n",
    "output_path_X_train = output_path + \"X_train_final\" + suffix + \".csv\"\n",
    "output_path_X_test  = output_path + \"X_test_final\"  + suffix + \".csv\"\n",
    "output_path_y_train = output_path + \"y_train_final\" + suffix + \".csv\"\n",
    "output_path_y_test  = output_path + \"y_test_final\"  + suffix + \".csv\"\n",
    "\n",
    "# Save all datasets\n",
    "X_train_final.to_csv(output_path_X_train, index=False)\n",
    "X_test_final.to_csv(output_path_X_test, index=False)\n",
    "y_train_final.to_csv(output_path_y_train, index=False)\n",
    "y_test_final.to_csv(output_path_y_test, index=False)\n",
    "\n",
    "print(\"- ‚úÖ Files saved with revision number:\", rev_number)\n",
    "print(\"- üí° Reminder: data/processed folder is ignored in .gitignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 16) PREDICTION MODELS\n",
    "# -------------------------------\n",
    "print(\"STEP 16) PREDICTION MODEL\")\n",
    "\n",
    "# |||||||||||||||||| \n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "rev_to_use = 0                            # Enter the desired revision number\n",
    "grid_cross_validation = 10                # Parameter cv for GridSearchCV\n",
    "grid_classification_scoring = \"accuracy\"  # Choose between: \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
    "grid_regression_scoring = \"r2\"            # Choose between: \"neg_root_mean_squared_error\", \"r2\"\n",
    "\n",
    "classification_models_selection = {\n",
    "    \"LogisticRegression\": False,\n",
    "    \"DecisionTreeClassifier\": False,\n",
    "    \"RandomForestClassifier\": True\n",
    "}\n",
    "\n",
    "regression_models_selection = {\n",
    "    \"LinearRegression\": False,\n",
    "    \"DecisionTreeRegressor\": False,\n",
    "    \"RandomForestRegressor\": True,\n",
    "    \"Lasso\": False,\n",
    "    \"Ridge\": False\n",
    "}\n",
    "\n",
    "classification_grids = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"penalty\": [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"max_depth\": [None, 5, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10, 20, 30]\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"n_estimators\": [5, 10, 20, 50, 100, 200],\n",
    "        \"max_depth\": [None, 2, 5, 10, 15, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "regression_grids = {\n",
    "    \"LinearRegression\": {},\n",
    "    \"Lasso\": {\n",
    "        \"alpha\": [0.001, 0.005, 0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 50],\n",
    "        \"max_iter\": [1000, 5000, 10000]\n",
    "    },\n",
    "    \"Ridge\": {\n",
    "        \"alpha\": [0.001, 0.005, 0.01, 0.1, 0.5, 1, 2, 5, 10, 20, 50],\n",
    "        \"max_iter\": [1000, 5000, 10000]\n",
    "    },\n",
    "    \"DecisionTreeRegressor\": {\n",
    "        \"criterion\": [\"squared_error\", \"friedman_mse\", \"absolute_error\", \"poisson\"],\n",
    "        \"max_depth\": [None, 5, 10, 20, 30],\n",
    "        \"min_samples_split\": [2, 5, 10, 20, 30]\n",
    "    },\n",
    "    \"RandomForestRegressor\": {\n",
    "        \"n_estimators\": [5, 10, 20, 50, 100, 200],\n",
    "        \"max_depth\": [None, 2, 5, 10, 15, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load processed data according to selected revision\n",
    "try:\n",
    "    X_train_model = pd.read_csv(output_path + \"X_train_final_\" + str(rev_to_use) + \".csv\")\n",
    "    X_test_model  = pd.read_csv(output_path + \"X_test_final_\" + str(rev_to_use) + \".csv\")\n",
    "    y_train_model = pd.read_csv(output_path + \"y_train_final_\" + str(rev_to_use) + \".csv\").squeeze()\n",
    "    y_test_model  = pd.read_csv(output_path + \"y_test_final_\" + str(rev_to_use) + \".csv\").squeeze()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(\n",
    "        f\"‚ùå The selected revision rev={rev_to_use} does NOT exist.\\n\"\n",
    "        f\"Missing file: {e.filename}\\n\"\n",
    "        f\"Please check available revision numbers in '../data/processed/'.\"\n",
    "    )\n",
    "print(f\"‚û°Ô∏è Loaded revision {rev_to_use}\")\n",
    "\n",
    "# Available models\n",
    "classification_available = {\n",
    "    \"LogisticRegression\": LogisticRegression(random_state=random_seed),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(random_state=random_seed),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=random_seed)\n",
    "}\n",
    "\n",
    "regression_available = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Lasso\": Lasso(random_state=random_seed),\n",
    "    \"Ridge\": Ridge(random_state=random_seed),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=random_seed),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=random_seed)\n",
    "}\n",
    "\n",
    "#  Auxiliary functions\n",
    "def compute_classification_metrics(y_true, y_pred, avg, pos_label):\n",
    "    metrics = {}\n",
    "    metrics[\"Accuracy\"]  = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"Precision\"] = precision_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
    "    metrics[\"Recall\"]    = recall_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
    "    metrics[\"F1_score\"]  = f1_score(y_true, y_pred, average=avg, pos_label=pos_label)\n",
    "    return metrics\n",
    "\n",
    "def compute_regression_metrics(y_true, y_pred):\n",
    "    metrics = {}\n",
    "    metrics[\"MAE\"]  = mean_absolute_error(y_true, y_pred)\n",
    "    metrics[\"MSE\"]  = mean_squared_error(y_true, y_pred)\n",
    "    metrics[\"RMSE\"] = np.sqrt(metrics[\"MSE\"])\n",
    "    metrics[\"R2\"]   = r2_score(y_true, y_pred)\n",
    "    return metrics\n",
    "\n",
    "def set_average_proposal(y):\n",
    "    unique_count = y.nunique()\n",
    "    if unique_count == 2:\n",
    "        freq = y.value_counts()\n",
    "        pos_label = freq.index[-1]\n",
    "        return \"binary\", pos_label, grid_classification_scoring\n",
    "\n",
    "    freq_norm = y.value_counts(normalize=True)\n",
    "    imbalance_ratio = freq_norm.max() / freq_norm.min()\n",
    "\n",
    "    if imbalance_ratio <= 1.2:\n",
    "        return \"micro\", None, grid_classification_scoring + \"_micro\" if grid_classification_scoring != \"accuracy\" else grid_classification_scoring\n",
    "    if imbalance_ratio <= 1.5:\n",
    "        return \"macro\", None, grid_classification_scoring + \"_macro\" if grid_classification_scoring != \"accuracy\" else grid_classification_scoring\n",
    "    return \"weighted\", None, grid_classification_scoring + \"_weighted\" if grid_classification_scoring != \"accuracy\" else grid_classification_scoring\n",
    "\n",
    "def left_align(df):\n",
    "    return df.style.set_table_styles(\n",
    "        [{'selector': 'th.row_heading', 'props': [('text-align', 'left')]}])\n",
    "\n",
    "# List with selected models\n",
    "classification_selected_list = []\n",
    "regression_selected_list = []\n",
    "for model_name, active in classification_models_selection.items():\n",
    "    if active:\n",
    "        classification_selected_list.append(model_name)\n",
    "for model_name, active in regression_models_selection.items():\n",
    "    if active:\n",
    "        regression_selected_list.append(model_name)\n",
    "\n",
    "# Auto-add Lasso/Ridge if LinearRegression is selected\n",
    "if \"LinearRegression\" in regression_selected_list:\n",
    "    if \"Lasso\" not in regression_selected_list:\n",
    "        regression_selected_list.append(\"Lasso\")\n",
    "    if \"Ridge\" not in regression_selected_list:\n",
    "        regression_selected_list.append(\"Ridge\")\n",
    "\n",
    "print(f\"\\n - Classification models selected: {classification_selected_list}\")\n",
    "print(f\" - Regression models selected:     {regression_selected_list}\")\n",
    "\n",
    "# Parameters needed for classification metrics\n",
    "if len(classification_selected_list) > 0:\n",
    "    proposed_avg, proposed_pos, proposed_score = set_average_proposal(y_train_model)\n",
    "\n",
    "# ======================================================\n",
    "#  CLASSIFICATION MODELS\n",
    "# ======================================================\n",
    "trained_models = {} \n",
    "default_results_class = {}\n",
    "optimized_results_class = {}\n",
    "for model_name in classification_selected_list:\n",
    "    # Instance DEFAULT model\n",
    "    default_model = classification_available[model_name] \n",
    "    # Train DEFAULT model\n",
    "    default_model.fit(X_train_model, y_train_model)\n",
    "    # Predict with trained DEFAULT model\n",
    "    y_pred_train = default_model.predict(X_train_model)\n",
    "    y_pred_test  = default_model.predict(X_test_model)\n",
    "    # Calculate metricts for DEFAULT model\n",
    "    metrics_train = compute_classification_metrics(y_train_model, y_pred_train, proposed_avg, proposed_pos)\n",
    "    metrics_test  = compute_classification_metrics(y_test_model, y_pred_test, proposed_avg, proposed_pos)\n",
    "    # Build final table with results for DEFAULT model\n",
    "    default_results_class[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = metrics_train\n",
    "    default_results_class[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = metrics_test\n",
    "    # Store trained model\n",
    "    trained_models[model_name] = {\"default\": default_model, \"optimized\": None}\n",
    "\n",
    "    # Set GRID parameters\n",
    "    grid_params = classification_grids[model_name]\n",
    "    if len(grid_params) > 0:\n",
    "        # Instance GRID\n",
    "        grid = GridSearchCV(\n",
    "            estimator = classification_available[model_name],\n",
    "            param_grid = grid_params,\n",
    "            scoring = proposed_score,\n",
    "            cv = grid_cross_validation)\n",
    "        # Train GRID\n",
    "        grid.fit(X_train_model, y_train_model)\n",
    "        # Get best estimator configuration (OPTIMIZED model)\n",
    "        best_model = grid.best_estimator_\n",
    "        # Predict with trained OPTIMIZED model\n",
    "        y_train_opt = best_model.predict(X_train_model)\n",
    "        y_test_opt  = best_model.predict(X_test_model)\n",
    "        # Calculate metricts for OPTIMIZED model\n",
    "        metrics_train_opt = compute_classification_metrics(y_train_model, y_train_opt, proposed_avg, proposed_pos)\n",
    "        metrics_test_opt  = compute_classification_metrics(y_test_model,  y_test_opt,  proposed_avg, proposed_pos)\n",
    "        # Build final table with results for OPTIMIZED model\n",
    "        optimized_results_class[\"OPTIMIZED \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train_opt, \"Best Parameters\": grid.best_params_, \"Scoring\": grid_classification_scoring}\n",
    "        optimized_results_class[\"OPTIMIZED \" + model_name + \" - üß™ TEST\"]  = {**metrics_test_opt,  \"Best Parameters\": grid.best_params_, \"Scoring\": grid_classification_scoring}\n",
    "        # Store trained model\n",
    "        trained_models[model_name][\"optimized\"] = best_model\n",
    "\n",
    "        # Store hyperparameters for RandomForestClassifier\n",
    "        if model_name == \"RandomForestClassifier\":\n",
    "            # Extract grid search raw values\n",
    "            est_values  = grid.cv_results_[\"param_n_estimators\"].data.astype(float)\n",
    "            depth_values = grid.cv_results_[\"param_max_depth\"].data\n",
    "            scores = grid.cv_results_[\"mean_test_score\"]\n",
    "            # Unique n_estimators\n",
    "            est_unique = np.sort(np.unique(est_values))\n",
    "            # Clean depths (convert to Python None or int)\n",
    "            cleaned_depths = []\n",
    "            for d in depth_values:\n",
    "                if d is None:\n",
    "                    cleaned_depths.append(None)\n",
    "                else:\n",
    "                    cleaned_depths.append(int(d))\n",
    "            # Unique depths excluding None\n",
    "            depth_unique = []\n",
    "            for d in cleaned_depths:\n",
    "                if d is not None and d not in depth_unique:\n",
    "                    depth_unique.append(d)\n",
    "            depth_unique = sorted(depth_unique)\n",
    "            # Dictionary: key = max_depth ‚Üí list of mean scores over n_estimators\n",
    "            depth_score_dict = {}\n",
    "            for d in depth_unique:\n",
    "                mean_scores = []\n",
    "                for e in est_unique:\n",
    "                    mask = (est_values == e) & (np.array(cleaned_depths, dtype=object) == d)\n",
    "                    mean_scores.append(scores[mask].mean())\n",
    "                depth_score_dict[d] = mean_scores\n",
    "            # Store internally\n",
    "            trained_models[model_name][\"est_list\"]   = est_unique\n",
    "            trained_models[model_name][\"depth_list\"] = depth_unique\n",
    "            trained_models[model_name][\"depth_score_dict\"] = depth_score_dict\n",
    "    else:\n",
    "        optimized_results_class[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train, \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
    "        optimized_results_class[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = {**metrics_test,  \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
    "\n",
    "# ======================================================\n",
    "#  REGRESSION MODELS\n",
    "# ======================================================\n",
    "default_results_reg = {}\n",
    "optimized_results_reg = {}\n",
    "for model_name in regression_selected_list:\n",
    "    # Instance DEFAULT model\n",
    "    default_model = regression_available[model_name]\n",
    "    # Train DEFAULT model\n",
    "    default_model.fit(X_train_model, y_train_model)\n",
    "    # Predict with trained DEFAULT model\n",
    "    y_pred_train = default_model.predict(X_train_model)\n",
    "    y_pred_test  = default_model.predict(X_test_model)\n",
    "    # Calculate metricts for DEFAULT model\n",
    "    metrics_train = compute_regression_metrics(y_train_model, y_pred_train)\n",
    "    metrics_test  = compute_regression_metrics(y_test_model, y_pred_test)\n",
    "    # Build final table with results for DEFAULT model\n",
    "    default_results_reg[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = metrics_train\n",
    "    default_results_reg[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = metrics_test\n",
    "    # Store trained model\n",
    "    trained_models[model_name] = {\"default\": default_model, \"optimized\": None}\n",
    "\n",
    "    # Set GRID parameters\n",
    "    grid_params = regression_grids[model_name]\n",
    "    if len(grid_params) > 0:\n",
    "        # Instance GRID\n",
    "        grid = GridSearchCV(\n",
    "            estimator = regression_available[model_name],\n",
    "            param_grid = grid_params,\n",
    "            scoring = grid_regression_scoring,\n",
    "            cv = grid_cross_validation)\n",
    "        # Train GRID\n",
    "        grid.fit(X_train_model, y_train_model)\n",
    "        # Get best estimator configuration (OPTIMIZED model)\n",
    "        best_model = grid.best_estimator_\n",
    "        # Predict with trained OPTIMIZED model\n",
    "        y_train_opt = best_model.predict(X_train_model)\n",
    "        y_test_opt  = best_model.predict(X_test_model)\n",
    "        # Calculate metricts for OPTIMIZED model\n",
    "        metrics_train_opt = compute_regression_metrics(y_train_model, y_train_opt)\n",
    "        metrics_test_opt  = compute_regression_metrics(y_test_model,  y_test_opt)\n",
    "        # Build final table with results for OPTIMIZED model\n",
    "        optimized_results_reg[\"OPTIMIZED \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train_opt, \"Best Parameters\": grid.best_params_, \"Scoring\": grid_regression_scoring}\n",
    "        optimized_results_reg[\"OPTIMIZED \" + model_name + \" - üß™ TEST\"]  = {**metrics_test_opt,  \"Best Parameters\": grid.best_params_, \"Scoring\": grid_regression_scoring}\n",
    "        # Store trained model\n",
    "        trained_models[model_name][\"optimized\"] = best_model\n",
    "\n",
    "        # Store hyperparameters for Lasso/Ridge\n",
    "        if model_name == \"Lasso\" or model_name == \"Ridge\":\n",
    "            # Extract grid search raw values\n",
    "            alpha_values = grid.cv_results_[\"param_alpha\"].data.astype(float)\n",
    "            scores = grid.cv_results_[\"mean_test_score\"]\n",
    "            # Compute the unique alpha values used in the grid (removing repetitions)\n",
    "            alpha_unique = np.sort(np.unique(alpha_values))\n",
    "            # Loop through each unique alpha value\n",
    "            mean_scores = []\n",
    "            for a in alpha_unique:\n",
    "                # Create a boolean mask\n",
    "                mask = (alpha_values == a)\n",
    "                # Compute the mean score for this alpha\n",
    "                mean_score = scores[mask].mean()\n",
    "                # Store the resulting mean score\n",
    "                mean_scores.append(mean_score)\n",
    "            # Store internally\n",
    "            trained_models[model_name][\"alpha_list\"] = alpha_unique\n",
    "            trained_models[model_name][\"alpha_scores\"] = mean_scores\n",
    "\n",
    "        # Store hyperparameters for RandomForest\n",
    "        if model_name == \"RandomForestRegressor\":\n",
    "            # Extract grid search raw values\n",
    "            est_values  = grid.cv_results_[\"param_n_estimators\"].data.astype(float)\n",
    "            depth_values = grid.cv_results_[\"param_max_depth\"].data\n",
    "            scores = grid.cv_results_[\"mean_test_score\"]\n",
    "            # Unique values of n_estimators\n",
    "            est_unique = np.sort(np.unique(est_values))\n",
    "            # Clean depths (convert to Python None or int)\n",
    "            cleaned_depths = []\n",
    "            for d in depth_values:\n",
    "                if d is None:\n",
    "                    cleaned_depths.append(None)\n",
    "                else:\n",
    "                    cleaned_depths.append(int(d))\n",
    "            # Unique valid depths (excluding None)\n",
    "            depth_unique = []\n",
    "            for d in cleaned_depths:\n",
    "                if d is not None and d not in depth_unique:\n",
    "                    depth_unique.append(d)\n",
    "            depth_unique = sorted(depth_unique)\n",
    "            # Dictionary: key=max_depth ‚Üí list of mean scores over n_estimators\n",
    "            depth_score_dict = {}\n",
    "            for d in depth_unique:\n",
    "                mean_scores = []\n",
    "                for e in est_unique:\n",
    "                    # Mask selecting rows matching both parameters\n",
    "                    mask = (est_values == e) & (np.array(cleaned_depths, dtype=object) == d)\n",
    "                    # Mean score for this (n_estimators, max_depth)\n",
    "                    mean_scores.append(scores[mask].mean())\n",
    "                depth_score_dict[d] = mean_scores\n",
    "            # Store internally\n",
    "            trained_models[model_name][\"est_list\"]   = est_unique\n",
    "            trained_models[model_name][\"depth_list\"] = depth_unique\n",
    "            trained_models[model_name][\"depth_score_dict\"] = depth_score_dict\n",
    "\n",
    "    else:\n",
    "        optimized_results_reg[\"DEFAULT \" + model_name + \" - üèãÔ∏è TRAIN\"] = {**metrics_train, \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
    "        optimized_results_reg[\"DEFAULT \" + model_name + \" - üß™ TEST\"]  = {**metrics_test,  \"Best Parameters\": \"N/A\", \"Scoring\": \"N/A\"}\n",
    "\n",
    "# ======================================================\n",
    "#  FINAL TABLES\n",
    "# ======================================================\n",
    "if len(classification_selected_list)>0:\n",
    "    print(\"\\n==================== ‚öñÔ∏è CLASSIFICATION MODELS COMPARISON ====================\")\n",
    "    display(left_align(pd.DataFrame(default_results_class).T))\n",
    "    display(left_align(pd.DataFrame(optimized_results_class).T))\n",
    "\n",
    "if len(regression_selected_list)>0:\n",
    "    print(\"\\n==================== üìà REGRESSION MODELS COMPARISON ========================\")\n",
    "    display(left_align(pd.DataFrame(default_results_reg).T))\n",
    "    display(left_align(pd.DataFrame(optimized_results_reg).T))\n",
    "\n",
    "# ======================================================\n",
    "# PLOTTING (DEFAULT vs OPTIMIZED)\n",
    "# ======================================================\n",
    "\n",
    "# Subplot template\n",
    "def subplot_template(nrows, ncols):\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(figWidth_unit * ncols, figHeight_unit * nrows))\n",
    "    return fig, ax\n",
    "\n",
    "# Lasso / Ridge hyperparameter plotter\n",
    "def plotter_Lasso_Ridge_hparame(model_name, grid_params, grid_scores, ax):\n",
    "    ax.plot(grid_params, grid_scores, marker=\"o\")\n",
    "    ax.set_xlabel(\"Alpha\", fontsize=my_font_size)\n",
    "    ax.set_ylabel(grid_regression_scoring, fontsize=my_font_size)\n",
    "    ax.set_title(f\"{model_name} - Alpha vs Scoring ({grid_regression_scoring})\", fontsize=font_size_titles)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "# Decision Tree plotter\n",
    "def plotter_DecisionTree_model(model, X, ax):\n",
    "    plot_tree(model, feature_names=X.columns, filled=True, ax=ax)\n",
    "\n",
    "# Generic RandomForest hyperparameter plotter (Classifier + Regressor)\n",
    "def plotter_RandomForest_hparam(model_name, est_list, depth_list, depth_score_dict, ax):\n",
    "    # Choose scoring label depending on model type\n",
    "    if model_name == \"RandomForestClassifier\":\n",
    "        scoring_label = grid_classification_scoring\n",
    "    else:\n",
    "        scoring_label = grid_regression_scoring\n",
    "    # Create a color palette (one color per n_estimators)\n",
    "    palette_colors = sns.color_palette(\"bright\", len(est_list))\n",
    "    # One curve per n_estimators\n",
    "    for i, est in enumerate(est_list):\n",
    "        # Retrieve scores for this n_estimators across all depths\n",
    "        est_scores = []\n",
    "        for d in depth_list:\n",
    "            est_scores.append(depth_score_dict[d][i])\n",
    "        # Plot curve: X = depth_list, Y = scores\n",
    "        ax.plot(depth_list, est_scores, marker=\"o\", color=palette_colors[i], label=f\"No. trees = {int(est)}\")\n",
    "    # Labels and formatting\n",
    "    ax.set_xlabel(\"Max depth\", fontsize=my_font_size)\n",
    "    ax.set_ylabel(scoring_label, fontsize=my_font_size)\n",
    "    ax.set_title(f\"{model_name} - Hyperparameter Analysis\", fontsize=font_size_titles)\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "\n",
    "# Registry of plotters\n",
    "model_plotters = {}\n",
    "model_plotters[\"Lasso\"] = plotter_Lasso_Ridge_hparame\n",
    "model_plotters[\"Ridge\"] = plotter_Lasso_Ridge_hparame\n",
    "model_plotters[\"DecisionTreeClassifier\"] = plotter_DecisionTree_model\n",
    "model_plotters[\"DecisionTreeRegressor\"]  = plotter_DecisionTree_model\n",
    "model_plotters[\"RandomForestClassifier\"] = plotter_RandomForest_hparam\n",
    "model_plotters[\"RandomForestRegressor\"] = plotter_RandomForest_hparam\n",
    "\n",
    "# ======================================================\n",
    "# EXECUTE PLOTS FOR MODELS THAT SUPPORT IT\n",
    "# ======================================================\n",
    "\n",
    "for model_name, model_dict in trained_models.items():\n",
    "    # Only plot models that have a registered plotter\n",
    "    if model_name not in model_plotters:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==================== üìä MODEL PLOT: {model_name} ====================\")\n",
    "\n",
    "    # If Lasso or Ridge ‚Üí use Lasso / Ridge hyperparameter plotter\n",
    "    if model_name in [\"Lasso\", \"Ridge\"]:\n",
    "        # Set values \n",
    "        alphas  = model_dict.get(\"alpha_list\", None)\n",
    "        scores  = model_dict.get(\"alpha_scores\", None)\n",
    "        if alphas is not None and scores is not None:\n",
    "            # Set subplot\n",
    "            fig, ax = subplot_template(nrows=1, ncols=1)\n",
    "            # Plot\n",
    "            plotter_Lasso_Ridge_hparame(model_name=model_name, grid_params=alphas, grid_scores=scores, ax=ax)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        continue\n",
    "\n",
    "    # If DecisionTree ‚Üí use Decision Tree plotter\n",
    "    if model_name in [\"DecisionTreeClassifier\", \"DecisionTreeRegressor\"]:\n",
    "        # Set subplot\n",
    "        fig, axes = subplot_template(nrows=1, ncols=2)\n",
    "        # Plot DEFAULT\n",
    "        plotter_DecisionTree_model(model=model_dict[\"default\"], X=X_train_model, ax=axes[0])\n",
    "        axes[0].set_title(f\"{model_name} - DEFAULT\", fontsize=font_size_titles)\n",
    "        # Plot OPTIMIZED\n",
    "        plotter_DecisionTree_model(model=model_dict[\"optimized\"], X=X_train_model, ax=axes[1])\n",
    "        axes[1].set_title(f\"{model_name} - OPTIMIZED\", fontsize=font_size_titles)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        continue\n",
    "\n",
    "    # If RandomForest ‚Üí use unified RandomForest hyperparameter plotter\n",
    "    if model_name in [\"RandomForestClassifier\", \"RandomForestRegressor\"]:\n",
    "        # Set values \n",
    "        est_list   = model_dict.get(\"est_list\", None)\n",
    "        depth_list = model_dict.get(\"depth_list\", None)\n",
    "        depth_score_dict = model_dict.get(\"depth_score_dict\", None)\n",
    "        if est_list is not None and depth_list is not None and depth_score_dict is not None:\n",
    "            # Set subplot\n",
    "            fig, ax = subplot_template(nrows=1, ncols=1)\n",
    "            # Plot\n",
    "            plotter_RandomForest_hparam(model_name=model_name, est_list=est_list, depth_list=depth_list, depth_score_dict=depth_score_dict, ax=ax)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88178137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 17) SAVE MODELS\n",
    "# -------------------------------\n",
    "print(\"STEP 17) SAVE MODELS\")\n",
    "\n",
    "# |||||||||||||||||| \n",
    "# ||||| INPUTS |||||\n",
    "# ||||||||||||||||||\n",
    "models_output_path = \"../models/\"   # Folder where models will be saved\n",
    "\n",
    "# Iterate over all trained models (default + optimized)\n",
    "for model_name, model_dict in trained_models.items():\n",
    "    # DEFAULT\n",
    "    if model_dict[\"default\"] is not None:\n",
    "        filename_default = models_output_path + f\"{model_name}_DEFAULT_{rev_to_use}.sav\"\n",
    "        dump(model_dict[\"default\"], open(filename_default, \"wb\"))\n",
    "\n",
    "    # OPTIMIZED\n",
    "    if model_dict[\"optimized\"] is not None:\n",
    "        filename_optimized = models_output_path + f\"{model_name}_OPTIMIZED_{rev_to_use}.sav\"\n",
    "        dump(model_dict[\"optimized\"], open(filename_optimized, \"wb\"))\n",
    "\n",
    "print(\"\\n‚úÖ All models saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
